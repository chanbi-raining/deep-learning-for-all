{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-variable linear regression\n",
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "y_data = [152., 185., 180., 196., 142.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for a tensor that will be always fed\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name = 'weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name = 'weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name = 'weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  6204.3867 \n",
      "Prediction:\n",
      " [231.89513 260.90475 266.42938 287.86945 196.47142]\n",
      "100 Cost:  51.856983 \n",
      "Prediction:\n",
      " [162.02919 177.46713 183.93793 198.07106 132.9358 ]\n",
      "200 Cost:  49.131607 \n",
      "Prediction:\n",
      " [161.75273 177.65656 183.85312 198.01051 133.18369]\n",
      "300 Cost:  46.549885 \n",
      "Prediction:\n",
      " [161.48366 177.84094 183.77055 197.95163 133.42494]\n",
      "400 Cost:  44.10435 \n",
      "Prediction:\n",
      " [161.22179 178.0204  183.69022 197.89435 133.65977]\n",
      "500 Cost:  41.78788 \n",
      "Prediction:\n",
      " [160.96687 178.19505 183.612   197.8386  133.88826]\n",
      "600 Cost:  39.593666 \n",
      "Prediction:\n",
      " [160.7188  178.36505 183.53587 197.78438 134.11064]\n",
      "700 Cost:  37.515083 \n",
      "Prediction:\n",
      " [160.47731 178.53049 183.46176 197.7316  134.32707]\n",
      "800 Cost:  35.54616 \n",
      "Prediction:\n",
      " [160.24228 178.69153 183.38965 197.68025 134.5377 ]\n",
      "900 Cost:  33.681164 \n",
      "Prediction:\n",
      " [160.01353 178.84827 183.31943 197.63031 134.74268]\n",
      "1000 Cost:  31.91452 \n",
      "Prediction:\n",
      " [159.79088 179.00081 183.25111 197.58171 134.94217]\n",
      "1100 Cost:  30.24111 \n",
      "Prediction:\n",
      " [159.57419 179.14928 183.1846  197.53444 135.13632]\n",
      "1200 Cost:  28.655895 \n",
      "Prediction:\n",
      " [159.36327 179.2938  183.11987 197.48842 135.32527]\n",
      "1300 Cost:  27.1544 \n",
      "Prediction:\n",
      " [159.15797 179.43442 183.05684 197.44366 135.50914]\n",
      "1400 Cost:  25.732025 \n",
      "Prediction:\n",
      " [158.95818 179.57133 182.99553 197.40013 135.68811]\n",
      "1500 Cost:  24.38475 \n",
      "Prediction:\n",
      " [158.7637  179.70457 182.93584 197.35777 135.86226]\n",
      "1600 Cost:  23.108469 \n",
      "Prediction:\n",
      " [158.5744  179.83424 182.87772 197.31656 136.03175]\n",
      "1700 Cost:  21.899542 \n",
      "Prediction:\n",
      " [158.39017 179.96046 182.82115 197.27647 136.1967 ]\n",
      "1800 Cost:  20.754421 \n",
      "Prediction:\n",
      " [158.21085 180.08331 182.7661  197.23746 136.35721]\n",
      "1900 Cost:  19.669683 \n",
      "Prediction:\n",
      " [158.03632 180.20288 182.71252 197.19952 136.51344]\n",
      "2000 Cost:  18.642145 \n",
      "Prediction:\n",
      " [157.86646 180.31927 182.66035 197.16261 136.66548]\n"
     ]
    }
   ],
   "source": [
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# minimize. assign a very small  learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\\\n",
    "                               feed_dict = {x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, 'Cost: ', cost_val, '\\nPrediction:\\n', hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  5082.118 \n",
      "Prediction:\n",
      " [[217.41205]\n",
      " [258.24326]\n",
      " [255.71935]\n",
      " [282.0083 ]\n",
      " [193.3462 ]]\n",
      "100 Cost:  14.779654 \n",
      "Prediction:\n",
      " [[154.1332 ]\n",
      " [182.41377]\n",
      " [180.88603]\n",
      " [200.51926]\n",
      " [135.5618 ]]\n",
      "200 Cost:  14.232534 \n",
      "Prediction:\n",
      " [[154.01682]\n",
      " [182.4946 ]\n",
      " [180.85168]\n",
      " [200.48409]\n",
      " [135.67657]]\n",
      "300 Cost:  13.713287 \n",
      "Prediction:\n",
      " [[153.9037 ]\n",
      " [182.5732 ]\n",
      " [180.81836]\n",
      " [200.44969]\n",
      " [135.78844]]\n",
      "400 Cost:  13.220227 \n",
      "Prediction:\n",
      " [[153.79367]\n",
      " [182.64964]\n",
      " [180.78596]\n",
      " [200.41597]\n",
      " [135.89745]]\n",
      "500 Cost:  12.75216 \n",
      "Prediction:\n",
      " [[153.68672]\n",
      " [182.724  ]\n",
      " [180.75453]\n",
      " [200.38297]\n",
      " [136.00368]]\n",
      "600 Cost:  12.307786 \n",
      "Prediction:\n",
      " [[153.58275]\n",
      " [182.79631]\n",
      " [180.72398]\n",
      " [200.35066]\n",
      " [136.1072 ]]\n",
      "700 Cost:  11.885674 \n",
      "Prediction:\n",
      " [[153.48169]\n",
      " [182.86665]\n",
      " [180.69434]\n",
      " [200.319  ]\n",
      " [136.20811]]\n",
      "800 Cost:  11.484792 \n",
      "Prediction:\n",
      " [[153.3834 ]\n",
      " [182.93504]\n",
      " [180.66551]\n",
      " [200.28798]\n",
      " [136.30646]]\n",
      "900 Cost:  11.104049 \n",
      "Prediction:\n",
      " [[153.28789]\n",
      " [183.00157]\n",
      " [180.63757]\n",
      " [200.25761]\n",
      " [136.40231]]\n",
      "1000 Cost:  10.742327 \n",
      "Prediction:\n",
      " [[153.19502]\n",
      " [183.06624]\n",
      " [180.61041]\n",
      " [200.22784]\n",
      " [136.49574]]\n",
      "1100 Cost:  10.398651 \n",
      "Prediction:\n",
      " [[153.10477]\n",
      " [183.12914]\n",
      " [180.58406]\n",
      " [200.19867]\n",
      " [136.5868 ]]\n",
      "1200 Cost:  10.072023 \n",
      "Prediction:\n",
      " [[153.01701]\n",
      " [183.1903 ]\n",
      " [180.55846]\n",
      " [200.17007]\n",
      " [136.67558]]\n",
      "1300 Cost:  9.761594 \n",
      "Prediction:\n",
      " [[152.93175]\n",
      " [183.24982]\n",
      " [180.53365]\n",
      " [200.14207]\n",
      " [136.76215]]\n",
      "1400 Cost:  9.466563 \n",
      "Prediction:\n",
      " [[152.84886]\n",
      " [183.30765]\n",
      " [180.50954]\n",
      " [200.11462]\n",
      " [136.84651]]\n",
      "1500 Cost:  9.186046 \n",
      "Prediction:\n",
      " [[152.7683 ]\n",
      " [183.36389]\n",
      " [180.48615]\n",
      " [200.08768]\n",
      " [136.92874]]\n",
      "1600 Cost:  8.9193 \n",
      "Prediction:\n",
      " [[152.68999]\n",
      " [183.41855]\n",
      " [180.46341]\n",
      " [200.06126]\n",
      " [137.00893]]\n",
      "1700 Cost:  8.665625 \n",
      "Prediction:\n",
      " [[152.61389]\n",
      " [183.47173]\n",
      " [180.44138]\n",
      " [200.03537]\n",
      " [137.0871 ]]\n",
      "1800 Cost:  8.424345 \n",
      "Prediction:\n",
      " [[152.53995]\n",
      " [183.5234 ]\n",
      " [180.42   ]\n",
      " [200.00998]\n",
      " [137.16331]]\n",
      "1900 Cost:  8.194758 \n",
      "Prediction:\n",
      " [[152.46808]\n",
      " [183.57368]\n",
      " [180.39926]\n",
      " [199.98508]\n",
      " [137.23764]]\n",
      "2000 Cost:  7.976293 \n",
      "Prediction:\n",
      " [[152.39825]\n",
      " [183.62254]\n",
      " [180.37914]\n",
      " [199.96063]\n",
      " [137.3101 ]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[ 73.,  80.,  75.],\n",
    "       [ 93.,  88.,  93.],\n",
    "       [ 89.,  91.,  90.],\n",
    "       [ 96.,  98., 100.],\n",
    "       [ 73.,  66.,  70.]]\n",
    "y_data = [[152.], [185.], [180.], [196.], [142.]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\\\n",
    "                               feed_dict = {X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, 'Cost: ', cost_val, '\\nPrediction:\\n', hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) 25\n",
      "(25, 1)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-01-test-score.csv', delimiter = ',', dtype = np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, len(x_data))\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  62817.01\n",
      "100 Cost:  11.267891\n",
      "200 Cost:  10.954045\n",
      "300 Cost:  10.661304\n",
      "400 Cost:  10.388141\n",
      "500 Cost:  10.133163\n",
      "600 Cost:  9.895073\n",
      "700 Cost:  9.672641\n",
      "800 Cost:  9.46479\n",
      "900 Cost:  9.270478\n",
      "1000 Cost:  9.088742\n",
      "1100 Cost:  8.918727\n",
      "1200 Cost:  8.75961\n",
      "1300 Cost:  8.610632\n",
      "1400 Cost:  8.471112\n",
      "1500 Cost:  8.340385\n",
      "1600 Cost:  8.21784\n",
      "1700 Cost:  8.102958\n",
      "1800 Cost:  7.9952016\n",
      "1900 Cost:  7.894087\n",
      "2000 Cost:  7.799204\n",
      "Your score will be  [[188.35426]]\n",
      "Their scores will be  [[171.88065]\n",
      " [177.35306]]\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\\\n",
    "                               feed_dict = {X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, 'Cost: ', cost_val)\n",
    "        #print('Prediction:\\n', hy_val)\n",
    "        \n",
    "# predict the scores\n",
    "print('Your score will be ', sess.run(hypothesis, feed_dict = {X: [[100, 70, 101]]}))\n",
    "print('Their scores will be ', sess.run(hypothesis, feed_dict = {X: [[60, 70, 110], [90, 100, 80]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queue Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(['data-01-test-score.csv'], \\\n",
    "                                                shuffle = False, name = 'filename_queue')\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# default values, in case of empty columns & specifies the type of the decoded result\n",
    "record_defaults = [[0.], [0.], [0.], [0.]]\n",
    "xy = tf.decode_csv(value, record_defaults = record_defaults)\n",
    "\n",
    "# collect batches of csv in\n",
    "train_x_batch, train_y_batch = tf.train.batch([xy[0:-1], xy[-1:]], batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  139692.4 \n",
      "Prediction:\n",
      " [[-216.35493]\n",
      " [-247.6725 ]\n",
      " [-250.58159]\n",
      " [-273.25534]\n",
      " [-185.07765]\n",
      " [-134.66534]\n",
      " [-207.85475]\n",
      " [-159.29553]\n",
      " [-228.56926]\n",
      " [-210.78374]]\n",
      "20 Cost:  48.285347 \n",
      "Prediction:\n",
      " [[148.57103 ]\n",
      " [190.94006 ]\n",
      " [181.58614 ]\n",
      " [197.42827 ]\n",
      " [149.42055 ]\n",
      " [111.97832 ]\n",
      " [144.497   ]\n",
      " [101.956406]\n",
      " [181.3713  ]\n",
      " [168.96603 ]]\n",
      "40 Cost:  47.891678 \n",
      "Prediction:\n",
      " [[148.61513]\n",
      " [190.92809]\n",
      " [181.60738]\n",
      " [197.4571 ]\n",
      " [149.39055]\n",
      " [111.95745]\n",
      " [144.54535]\n",
      " [102.03159]\n",
      " [181.35204]\n",
      " [168.95506]]\n",
      "60 Cost:  47.43516 \n",
      "Prediction:\n",
      " [[148.64246 ]\n",
      " [190.89632 ]\n",
      " [181.6089  ]\n",
      " [197.46448 ]\n",
      " [149.34554 ]\n",
      " [111.925545]\n",
      " [144.5775  ]\n",
      " [102.0946  ]\n",
      " [181.31435 ]\n",
      " [168.927   ]]\n",
      "80 Cost:  46.983482 \n",
      "Prediction:\n",
      " [[148.66966 ]\n",
      " [190.86475 ]\n",
      " [181.61043 ]\n",
      " [197.47186 ]\n",
      " [149.30078 ]\n",
      " [111.893814]\n",
      " [144.60953 ]\n",
      " [102.1573  ]\n",
      " [181.27689 ]\n",
      " [168.89914 ]]\n",
      "100 Cost:  46.53667 \n",
      "Prediction:\n",
      " [[148.69669]\n",
      " [190.83333]\n",
      " [181.61195]\n",
      " [197.47917]\n",
      " [149.25626]\n",
      " [111.86229]\n",
      " [144.6414 ]\n",
      " [102.2197 ]\n",
      " [181.23965]\n",
      " [168.8715 ]]\n",
      "120 Cost:  46.094475 \n",
      "Prediction:\n",
      " [[148.72354]\n",
      " [190.80206]\n",
      " [181.61342]\n",
      " [197.48643]\n",
      " [149.21194]\n",
      " [111.83094]\n",
      " [144.6731 ]\n",
      " [102.28181]\n",
      " [181.2026 ]\n",
      " [168.84404]]\n",
      "140 Cost:  45.65702 \n",
      "Prediction:\n",
      " [[148.75026 ]\n",
      " [190.77098 ]\n",
      " [181.6149  ]\n",
      " [197.49368 ]\n",
      " [149.16788 ]\n",
      " [111.799774]\n",
      " [144.70467 ]\n",
      " [102.343605]\n",
      " [181.1658  ]\n",
      " [168.81677 ]]\n",
      "160 Cost:  45.224144 \n",
      "Prediction:\n",
      " [[148.77682 ]\n",
      " [190.74007 ]\n",
      " [181.61635 ]\n",
      " [197.5009  ]\n",
      " [149.12404 ]\n",
      " [111.768776]\n",
      " [144.73607 ]\n",
      " [102.40511 ]\n",
      " [181.1292  ]\n",
      " [168.78969 ]]\n",
      "180 Cost:  44.79584 \n",
      "Prediction:\n",
      " [[148.80322 ]\n",
      " [190.7093  ]\n",
      " [181.61778 ]\n",
      " [197.50807 ]\n",
      " [149.0804  ]\n",
      " [111.737976]\n",
      " [144.76732 ]\n",
      " [102.46633 ]\n",
      " [181.09282 ]\n",
      " [168.7628  ]]\n",
      "200 Cost:  44.372124 \n",
      "Prediction:\n",
      " [[148.8295 ]\n",
      " [190.67871]\n",
      " [181.61923]\n",
      " [197.5152 ]\n",
      " [149.03702]\n",
      " [111.70737]\n",
      " [144.79843]\n",
      " [102.52725]\n",
      " [181.05664]\n",
      " [168.73611]]\n",
      "220 Cost:  43.952812 \n",
      "Prediction:\n",
      " [[148.85559]\n",
      " [190.64828]\n",
      " [181.62062]\n",
      " [197.52231]\n",
      " [148.99385]\n",
      " [111.6769 ]\n",
      " [144.8294 ]\n",
      " [102.58787]\n",
      " [181.02068]\n",
      " [168.7096 ]]\n",
      "240 Cost:  43.53794 \n",
      "Prediction:\n",
      " [[148.88152]\n",
      " [190.618  ]\n",
      " [181.62201]\n",
      " [197.52939]\n",
      " [148.95088]\n",
      " [111.64664]\n",
      " [144.86021]\n",
      " [102.64821]\n",
      " [180.98494]\n",
      " [168.68326]]\n",
      "260 Cost:  43.12757 \n",
      "Prediction:\n",
      " [[148.90735 ]\n",
      " [190.58792 ]\n",
      " [181.62341 ]\n",
      " [197.53642 ]\n",
      " [148.90819 ]\n",
      " [111.616554]\n",
      " [144.89088 ]\n",
      " [102.70826 ]\n",
      " [180.9494  ]\n",
      " [168.65714 ]]\n",
      "280 Cost:  42.721397 \n",
      "Prediction:\n",
      " [[148.93298]\n",
      " [190.55795]\n",
      " [181.62476]\n",
      " [197.54343]\n",
      " [148.86568]\n",
      " [111.58663]\n",
      " [144.9214 ]\n",
      " [102.76802]\n",
      " [180.91406]\n",
      " [168.63118]]\n",
      "300 Cost:  42.31962 \n",
      "Prediction:\n",
      " [[148.95851]\n",
      " [190.5282 ]\n",
      " [181.62611]\n",
      " [197.55042]\n",
      " [148.8234 ]\n",
      " [111.5569 ]\n",
      " [144.95178]\n",
      " [102.8275 ]\n",
      " [180.87894]\n",
      " [168.60542]]\n",
      "320 Cost:  41.92198 \n",
      "Prediction:\n",
      " [[148.98384]\n",
      " [190.49855]\n",
      " [181.62744]\n",
      " [197.55733]\n",
      " [148.78133]\n",
      " [111.52732]\n",
      " [144.98198]\n",
      " [102.88668]\n",
      " [180.844  ]\n",
      " [168.5798 ]]\n",
      "340 Cost:  41.52871 \n",
      "Prediction:\n",
      " [[149.00906]\n",
      " [190.46912]\n",
      " [181.62877]\n",
      " [197.56427]\n",
      " [148.73949]\n",
      " [111.49793]\n",
      " [145.01208]\n",
      " [102.94559]\n",
      " [180.8093 ]\n",
      " [168.55443]]\n",
      "360 Cost:  41.13949 \n",
      "Prediction:\n",
      " [[149.03415]\n",
      " [190.43979]\n",
      " [181.63008]\n",
      " [197.57114]\n",
      " [148.69786]\n",
      " [111.46871]\n",
      " [145.04204]\n",
      " [103.0042 ]\n",
      " [180.7748 ]\n",
      " [168.52919]]\n",
      "380 Cost:  40.754402 \n",
      "Prediction:\n",
      " [[149.05907 ]\n",
      " [190.41064 ]\n",
      " [181.63138 ]\n",
      " [197.57797 ]\n",
      " [148.65645 ]\n",
      " [111.43966 ]\n",
      " [145.07182 ]\n",
      " [103.062546]\n",
      " [180.74048 ]\n",
      " [168.50414 ]]\n",
      "400 Cost:  40.37335 \n",
      "Prediction:\n",
      " [[149.08386 ]\n",
      " [190.38164 ]\n",
      " [181.63268 ]\n",
      " [197.58478 ]\n",
      " [148.61526 ]\n",
      " [111.410774]\n",
      " [145.1015  ]\n",
      " [103.1206  ]\n",
      " [180.70634 ]\n",
      " [168.47925 ]]\n",
      "420 Cost:  39.996407 \n",
      "Prediction:\n",
      " [[149.1085  ]\n",
      " [190.35281 ]\n",
      " [181.63393 ]\n",
      " [197.59154 ]\n",
      " [148.57428 ]\n",
      " [111.382065]\n",
      " [145.13101 ]\n",
      " [103.178375]\n",
      " [180.67244 ]\n",
      " [168.45456 ]]\n",
      "440 Cost:  39.62336 \n",
      "Prediction:\n",
      " [[149.13301]\n",
      " [190.32413]\n",
      " [181.6352 ]\n",
      " [197.5983 ]\n",
      " [148.5335 ]\n",
      " [111.35351]\n",
      " [145.16037]\n",
      " [103.23588]\n",
      " [180.63872]\n",
      " [168.43002]]\n",
      "460 Cost:  39.25428 \n",
      "Prediction:\n",
      " [[149.15736 ]\n",
      " [190.29558 ]\n",
      " [181.63641 ]\n",
      " [197.605   ]\n",
      " [148.49294 ]\n",
      " [111.325134]\n",
      " [145.18959 ]\n",
      " [103.2931  ]\n",
      " [180.60518 ]\n",
      " [168.40565 ]]\n",
      "480 Cost:  38.88911 \n",
      "Prediction:\n",
      " [[149.1816 ]\n",
      " [190.26721]\n",
      " [181.63766]\n",
      " [197.6117 ]\n",
      " [148.45259]\n",
      " [111.29691]\n",
      " [145.2187 ]\n",
      " [103.35006]\n",
      " [180.57187]\n",
      " [168.38147]]\n",
      "500 Cost:  38.52775 \n",
      "Prediction:\n",
      " [[149.20566]\n",
      " [190.23897]\n",
      " [181.63885]\n",
      " [197.61832]\n",
      " [148.41245]\n",
      " [111.26885]\n",
      " [145.24765]\n",
      " [103.40672]\n",
      " [180.53871]\n",
      " [168.35744]]\n",
      "520 Cost:  38.17025 \n",
      "Prediction:\n",
      " [[149.22961]\n",
      " [190.21088]\n",
      " [181.64006]\n",
      " [197.62494]\n",
      " [148.37251]\n",
      " [111.24096]\n",
      " [145.27646]\n",
      " [103.46313]\n",
      " [180.50578]\n",
      " [168.33359]]\n",
      "540 Cost:  37.81653 \n",
      "Prediction:\n",
      " [[149.25343]\n",
      " [190.18297]\n",
      " [181.64125]\n",
      " [197.63153]\n",
      " [148.33278]\n",
      " [111.21324]\n",
      " [145.30515]\n",
      " [103.51927]\n",
      " [180.47304]\n",
      " [168.3099 ]]\n",
      "560 Cost:  37.46659 \n",
      "Prediction:\n",
      " [[149.2771 ]\n",
      " [190.15518]\n",
      " [181.64243]\n",
      " [197.63809]\n",
      " [148.29327]\n",
      " [111.18568]\n",
      " [145.33368]\n",
      " [103.57513]\n",
      " [180.44049]\n",
      " [168.28638]]\n",
      "580 Cost:  37.120293 \n",
      "Prediction:\n",
      " [[149.30063]\n",
      " [190.12756]\n",
      " [181.64359]\n",
      " [197.64459]\n",
      " [148.25395]\n",
      " [111.15827]\n",
      " [145.36209]\n",
      " [103.63072]\n",
      " [180.40811]\n",
      " [168.26303]]\n",
      "600 Cost:  36.777576 \n",
      "Prediction:\n",
      " [[149.32404 ]\n",
      " [190.10007 ]\n",
      " [181.64473 ]\n",
      " [197.65108 ]\n",
      " [148.21483 ]\n",
      " [111.131004]\n",
      " [145.39035 ]\n",
      " [103.68604 ]\n",
      " [180.37592 ]\n",
      " [168.23982 ]]\n",
      "620 Cost:  36.43854 \n",
      "Prediction:\n",
      " [[149.3473 ]\n",
      " [190.07271]\n",
      " [181.64586]\n",
      " [197.65755]\n",
      " [148.17592]\n",
      " [111.10393]\n",
      " [145.41847]\n",
      " [103.74111]\n",
      " [180.34392]\n",
      " [168.21678]]\n",
      "640 Cost:  36.103027 \n",
      "Prediction:\n",
      " [[149.37042 ]\n",
      " [190.0455  ]\n",
      " [181.64697 ]\n",
      " [197.66396 ]\n",
      " [148.13719 ]\n",
      " [111.07699 ]\n",
      " [145.44646 ]\n",
      " [103.795906]\n",
      " [180.31209 ]\n",
      " [168.1939  ]]\n",
      "660 Cost:  35.771126 \n",
      "Prediction:\n",
      " [[149.39345]\n",
      " [190.01846]\n",
      " [181.6481 ]\n",
      " [197.67035]\n",
      " [148.09868]\n",
      " [111.0502 ]\n",
      " [145.47433]\n",
      " [103.85043]\n",
      " [180.28047]\n",
      " [168.1712 ]]\n",
      "680 Cost:  35.442688 \n",
      "Prediction:\n",
      " [[149.41632 ]\n",
      " [189.99155 ]\n",
      " [181.64919 ]\n",
      " [197.67673 ]\n",
      " [148.06036 ]\n",
      " [111.023575]\n",
      " [145.50204 ]\n",
      " [103.90469 ]\n",
      " [180.24901 ]\n",
      " [168.14862 ]]\n",
      "700 Cost:  35.117786 \n",
      "Prediction:\n",
      " [[149.43907]\n",
      " [189.9648 ]\n",
      " [181.65028]\n",
      " [197.68306]\n",
      " [148.02226]\n",
      " [110.99711]\n",
      " [145.52966]\n",
      " [103.9587 ]\n",
      " [180.21776]\n",
      " [168.12624]]\n",
      "720 Cost:  34.79622 \n",
      "Prediction:\n",
      " [[149.46167 ]\n",
      " [189.93816 ]\n",
      " [181.65134 ]\n",
      " [197.68936 ]\n",
      " [147.98433 ]\n",
      " [110.970795]\n",
      " [145.55711 ]\n",
      " [104.01245 ]\n",
      " [180.18666 ]\n",
      " [168.10399 ]]\n",
      "740 Cost:  34.478035 \n",
      "Prediction:\n",
      " [[149.48415]\n",
      " [189.91167]\n",
      " [181.65239]\n",
      " [197.69562]\n",
      " [147.94661]\n",
      " [110.94462]\n",
      " [145.58443]\n",
      " [104.06594]\n",
      " [180.15575]\n",
      " [168.08188]]\n",
      "760 Cost:  34.163326 \n",
      "Prediction:\n",
      " [[149.50652]\n",
      " [189.88535]\n",
      " [181.65347]\n",
      " [197.70189]\n",
      " [147.90909]\n",
      " [110.91861]\n",
      " [145.61166]\n",
      " [104.11917]\n",
      " [180.12505]\n",
      " [168.05997]]\n",
      "780 Cost:  33.8518 \n",
      "Prediction:\n",
      " [[149.52875]\n",
      " [189.85913]\n",
      " [181.65448]\n",
      " [197.70808]\n",
      " [147.87175]\n",
      " [110.89273]\n",
      " [145.6387 ]\n",
      " [104.17214]\n",
      " [180.09448]\n",
      " [168.03816]]\n",
      "800 Cost:  33.543674 \n",
      "Prediction:\n",
      " [[149.55086]\n",
      " [189.83308]\n",
      " [181.65553]\n",
      " [197.71428]\n",
      " [147.83461]\n",
      " [110.86703]\n",
      " [145.66566]\n",
      " [104.22486]\n",
      " [180.06412]\n",
      " [168.01654]]\n",
      "820 Cost:  33.23876 \n",
      "Prediction:\n",
      " [[149.57283]\n",
      " [189.80716]\n",
      " [181.65656]\n",
      " [197.72043]\n",
      " [147.79767]\n",
      " [110.84146]\n",
      " [145.69246]\n",
      " [104.27733]\n",
      " [180.03392]\n",
      " [167.99506]]\n",
      "840 Cost:  32.936943 \n",
      "Prediction:\n",
      " [[149.59467]\n",
      " [189.78136]\n",
      " [181.65753]\n",
      " [197.72653]\n",
      " [147.76088]\n",
      " [110.81603]\n",
      " [145.71913]\n",
      " [104.32954]\n",
      " [180.00388]\n",
      " [167.9737 ]]\n",
      "860 Cost:  32.638386 \n",
      "Prediction:\n",
      " [[149.61641]\n",
      " [189.7557 ]\n",
      " [181.65854]\n",
      " [197.73264]\n",
      " [147.72432]\n",
      " [110.79076]\n",
      " [145.7457 ]\n",
      " [104.38151]\n",
      " [179.97403]\n",
      " [167.9525 ]]\n",
      "880 Cost:  32.343037 \n",
      "Prediction:\n",
      " [[149.63802]\n",
      " [189.73021]\n",
      " [181.65953]\n",
      " [197.73871]\n",
      " [147.68794]\n",
      " [110.76563]\n",
      " [145.77213]\n",
      " [104.43322]\n",
      " [179.94435]\n",
      " [167.93147]]\n",
      "900 Cost:  32.050747 \n",
      "Prediction:\n",
      " [[149.6595  ]\n",
      " [189.7048  ]\n",
      " [181.66049 ]\n",
      " [197.74475 ]\n",
      " [147.65175 ]\n",
      " [110.74065 ]\n",
      " [145.79842 ]\n",
      " [104.484665]\n",
      " [179.91484 ]\n",
      " [167.91055 ]]\n",
      "920 Cost:  31.761578 \n",
      "Prediction:\n",
      " [[149.68086]\n",
      " [189.67958]\n",
      " [181.66145]\n",
      " [197.75075]\n",
      " [147.61574]\n",
      " [110.71581]\n",
      " [145.8246 ]\n",
      " [104.53588]\n",
      " [179.88551]\n",
      " [167.88982]]\n",
      "940 Cost:  31.47544 \n",
      "Prediction:\n",
      " [[149.7021  ]\n",
      " [189.65446 ]\n",
      " [181.6624  ]\n",
      " [197.75673 ]\n",
      " [147.57993 ]\n",
      " [110.69112 ]\n",
      " [145.85063 ]\n",
      " [104.586845]\n",
      " [179.85634 ]\n",
      " [167.8692  ]]\n",
      "960 Cost:  31.192242 \n",
      "Prediction:\n",
      " [[149.72322]\n",
      " [189.62949]\n",
      " [181.66333]\n",
      " [197.76265]\n",
      " [147.54427]\n",
      " [110.66655]\n",
      " [145.87654]\n",
      " [104.63755]\n",
      " [179.82732]\n",
      " [167.84871]]\n",
      "980 Cost:  30.912083 \n",
      "Prediction:\n",
      " [[149.74422]\n",
      " [189.60463]\n",
      " [181.66426]\n",
      " [197.7686 ]\n",
      " [147.50882]\n",
      " [110.64214]\n",
      " [145.90236]\n",
      " [104.68804]\n",
      " [179.7985 ]\n",
      " [167.82838]]\n",
      "1000 Cost:  30.634953 \n",
      "Prediction:\n",
      " [[149.7651 ]\n",
      " [189.57994]\n",
      " [181.6652 ]\n",
      " [197.7745 ]\n",
      " [147.47357]\n",
      " [110.61787]\n",
      " [145.92804]\n",
      " [104.73827]\n",
      " [179.76984]\n",
      " [167.80821]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020 Cost:  30.360638 \n",
      "Prediction:\n",
      " [[149.78586 ]\n",
      " [189.55536 ]\n",
      " [181.66609 ]\n",
      " [197.78035 ]\n",
      " [147.43846 ]\n",
      " [110.593735]\n",
      " [145.95358 ]\n",
      " [104.78825 ]\n",
      " [179.74132 ]\n",
      " [167.78815 ]]\n",
      "1040 Cost:  30.089197 \n",
      "Prediction:\n",
      " [[149.8065  ]\n",
      " [189.53088 ]\n",
      " [181.66699 ]\n",
      " [197.7862  ]\n",
      " [147.40355 ]\n",
      " [110.569725]\n",
      " [145.97899 ]\n",
      " [104.838   ]\n",
      " [179.71295 ]\n",
      " [167.76823 ]]\n",
      "1060 Cost:  29.820675 \n",
      "Prediction:\n",
      " [[149.82704 ]\n",
      " [189.50656 ]\n",
      " [181.66788 ]\n",
      " [197.79199 ]\n",
      " [147.36882 ]\n",
      " [110.545876]\n",
      " [146.0043  ]\n",
      " [104.887505]\n",
      " [179.68478 ]\n",
      " [167.74846 ]]\n",
      "1080 Cost:  29.555014 \n",
      "Prediction:\n",
      " [[149.84746 ]\n",
      " [189.48238 ]\n",
      " [181.66878 ]\n",
      " [197.7978  ]\n",
      " [147.33427 ]\n",
      " [110.522156]\n",
      " [146.0295  ]\n",
      " [104.936775]\n",
      " [179.65678 ]\n",
      " [167.72882 ]]\n",
      "1100 Cost:  29.292048 \n",
      "Prediction:\n",
      " [[149.86775 ]\n",
      " [189.4583  ]\n",
      " [181.66963 ]\n",
      " [197.80353 ]\n",
      " [147.2999  ]\n",
      " [110.498566]\n",
      " [146.05455 ]\n",
      " [104.98581 ]\n",
      " [179.62892 ]\n",
      " [167.7093  ]]\n",
      "1120 Cost:  29.031912 \n",
      "Prediction:\n",
      " [[149.88794]\n",
      " [189.43437]\n",
      " [181.67049]\n",
      " [197.80927]\n",
      " [147.2657 ]\n",
      " [110.47511]\n",
      " [146.0795 ]\n",
      " [105.0346 ]\n",
      " [179.60121]\n",
      " [167.68994]]\n",
      "1140 Cost:  28.774517 \n",
      "Prediction:\n",
      " [[149.90799 ]\n",
      " [189.41055 ]\n",
      " [181.67133 ]\n",
      " [197.81496 ]\n",
      " [147.23169 ]\n",
      " [110.45178 ]\n",
      " [146.10431 ]\n",
      " [105.083145]\n",
      " [179.57367 ]\n",
      " [167.6707  ]]\n",
      "1160 Cost:  28.519796 \n",
      "Prediction:\n",
      " [[149.92795]\n",
      " [189.38686]\n",
      " [181.67216]\n",
      " [197.8206 ]\n",
      " [147.19785]\n",
      " [110.42861]\n",
      " [146.12901]\n",
      " [105.13148]\n",
      " [179.54628]\n",
      " [167.65158]]\n",
      "1180 Cost:  28.267857 \n",
      "Prediction:\n",
      " [[149.94781 ]\n",
      " [189.36331 ]\n",
      " [181.67302 ]\n",
      " [197.82628 ]\n",
      " [147.16418 ]\n",
      " [110.40556 ]\n",
      " [146.15361 ]\n",
      " [105.179565]\n",
      " [179.51907 ]\n",
      " [167.63263 ]]\n",
      "1200 Cost:  28.018509 \n",
      "Prediction:\n",
      " [[149.96753 ]\n",
      " [189.33987 ]\n",
      " [181.67383 ]\n",
      " [197.8319  ]\n",
      " [147.13069 ]\n",
      " [110.382645]\n",
      " [146.17807 ]\n",
      " [105.22742 ]\n",
      " [179.492   ]\n",
      " [167.61377 ]]\n",
      "1220 Cost:  27.771765 \n",
      "Prediction:\n",
      " [[149.98717 ]\n",
      " [189.31657 ]\n",
      " [181.67465 ]\n",
      " [197.83748 ]\n",
      " [147.09737 ]\n",
      " [110.359856]\n",
      " [146.20242 ]\n",
      " [105.27505 ]\n",
      " [179.46509 ]\n",
      " [167.59505 ]]\n",
      "1240 Cost:  27.527607 \n",
      "Prediction:\n",
      " [[150.00665]\n",
      " [189.29337]\n",
      " [181.67545]\n",
      " [197.84305]\n",
      " [147.06421]\n",
      " [110.33719]\n",
      " [146.22664]\n",
      " [105.32243]\n",
      " [179.43831]\n",
      " [167.57645]]\n",
      "1260 Cost:  27.286036 \n",
      "Prediction:\n",
      " [[150.02606]\n",
      " [189.2703 ]\n",
      " [181.67624]\n",
      " [197.84857]\n",
      " [147.03122]\n",
      " [110.31466]\n",
      " [146.25075]\n",
      " [105.3696 ]\n",
      " [179.41171]\n",
      " [167.55798]]\n",
      "1280 Cost:  27.046988 \n",
      "Prediction:\n",
      " [[150.04535]\n",
      " [189.24734]\n",
      " [181.67702]\n",
      " [197.85406]\n",
      " [146.99841]\n",
      " [110.29226]\n",
      " [146.27473]\n",
      " [105.41654]\n",
      " [179.38524]\n",
      " [167.53964]]\n",
      "1300 Cost:  26.810492 \n",
      "Prediction:\n",
      " [[150.06454]\n",
      " [189.22452]\n",
      " [181.6778 ]\n",
      " [197.85956]\n",
      " [146.96576]\n",
      " [110.26999]\n",
      " [146.29861]\n",
      " [105.46325]\n",
      " [179.35895]\n",
      " [167.52142]]\n",
      "1320 Cost:  26.576513 \n",
      "Prediction:\n",
      " [[150.08362]\n",
      " [189.20184]\n",
      " [181.67857]\n",
      " [197.86502]\n",
      " [146.9333 ]\n",
      " [110.24783]\n",
      " [146.3224 ]\n",
      " [105.50973]\n",
      " [179.3328 ]\n",
      " [167.50334]]\n",
      "1340 Cost:  26.344952 \n",
      "Prediction:\n",
      " [[150.10258]\n",
      " [189.17926]\n",
      " [181.67934]\n",
      " [197.87045]\n",
      " [146.90099]\n",
      " [110.22583]\n",
      " [146.34604]\n",
      " [105.556  ]\n",
      " [179.30678]\n",
      " [167.48537]]\n",
      "1360 Cost:  26.115799 \n",
      "Prediction:\n",
      " [[150.12144]\n",
      " [189.15678]\n",
      " [181.68007]\n",
      " [197.87584]\n",
      " [146.86884]\n",
      " [110.20392]\n",
      " [146.36957]\n",
      " [105.60202]\n",
      " [179.28091]\n",
      " [167.46751]]\n",
      "1380 Cost:  25.88909 \n",
      "Prediction:\n",
      " [[150.14018 ]\n",
      " [189.13443 ]\n",
      " [181.6808  ]\n",
      " [197.88121 ]\n",
      " [146.83685 ]\n",
      " [110.18215 ]\n",
      " [146.39299 ]\n",
      " [105.647835]\n",
      " [179.2552  ]\n",
      " [167.4498  ]]\n",
      "1400 Cost:  25.66476 \n",
      "Prediction:\n",
      " [[150.15884 ]\n",
      " [189.1122  ]\n",
      " [181.68153 ]\n",
      " [197.88657 ]\n",
      " [146.80505 ]\n",
      " [110.160515]\n",
      " [146.4163  ]\n",
      " [105.693436]\n",
      " [179.22963 ]\n",
      " [167.43219 ]]\n",
      "1420 Cost:  25.442827 \n",
      "Prediction:\n",
      " [[150.1774 ]\n",
      " [189.09009]\n",
      " [181.68228]\n",
      " [197.8919 ]\n",
      " [146.77342]\n",
      " [110.13898]\n",
      " [146.43951]\n",
      " [105.7388 ]\n",
      " [179.20422]\n",
      " [167.41472]]\n",
      "1440 Cost:  25.223207 \n",
      "Prediction:\n",
      " [[150.19583]\n",
      " [189.06812]\n",
      " [181.68298]\n",
      " [197.8972 ]\n",
      " [146.74191]\n",
      " [110.11758]\n",
      " [146.4626 ]\n",
      " [105.78395]\n",
      " [179.17896]\n",
      " [167.39737]]\n",
      "1460 Cost:  25.005932 \n",
      "Prediction:\n",
      " [[150.21417]\n",
      " [189.04623]\n",
      " [181.68372]\n",
      " [197.90248]\n",
      " [146.7106 ]\n",
      " [110.09631]\n",
      " [146.48557]\n",
      " [105.82889]\n",
      " [179.15382]\n",
      " [167.38011]]\n",
      "1480 Cost:  24.790867 \n",
      "Prediction:\n",
      " [[150.2324  ]\n",
      " [189.02448 ]\n",
      " [181.68439 ]\n",
      " [197.90771 ]\n",
      " [146.67944 ]\n",
      " [110.07515 ]\n",
      " [146.50844 ]\n",
      " [105.873604]\n",
      " [179.12883 ]\n",
      " [167.36298 ]]\n",
      "1500 Cost:  24.578068 \n",
      "Prediction:\n",
      " [[150.25053 ]\n",
      " [189.0028  ]\n",
      " [181.68509 ]\n",
      " [197.91293 ]\n",
      " [146.64844 ]\n",
      " [110.05411 ]\n",
      " [146.53119 ]\n",
      " [105.918106]\n",
      " [179.10397 ]\n",
      " [167.34596 ]]\n",
      "1520 Cost:  24.367527 \n",
      "Prediction:\n",
      " [[150.26855]\n",
      " [188.98126]\n",
      " [181.68576]\n",
      " [197.91812]\n",
      " [146.61758]\n",
      " [110.03318]\n",
      " [146.55383]\n",
      " [105.96237]\n",
      " [179.07927]\n",
      " [167.32906]]\n",
      "1540 Cost:  24.15917 \n",
      "Prediction:\n",
      " [[150.28648 ]\n",
      " [188.95984 ]\n",
      " [181.68643 ]\n",
      " [197.9233  ]\n",
      " [146.58688 ]\n",
      " [110.012375]\n",
      " [146.57635 ]\n",
      " [106.00644 ]\n",
      " [179.05469 ]\n",
      " [167.31226 ]]\n",
      "1560 Cost:  23.953083 \n",
      "Prediction:\n",
      " [[150.30432]\n",
      " [188.93854]\n",
      " [181.6871 ]\n",
      " [197.92844]\n",
      " [146.55637]\n",
      " [109.99171]\n",
      " [146.59879]\n",
      " [106.0503 ]\n",
      " [179.03027]\n",
      " [167.2956 ]]\n",
      "1580 Cost:  23.749111 \n",
      "Prediction:\n",
      " [[150.32205]\n",
      " [188.91733]\n",
      " [181.68776]\n",
      " [197.93356]\n",
      " [146.52599]\n",
      " [109.97114]\n",
      " [146.6211 ]\n",
      " [106.09394]\n",
      " [179.00598]\n",
      " [167.27904]]\n",
      "1600 Cost:  23.547293 \n",
      "Prediction:\n",
      " [[150.33968 ]\n",
      " [188.89626 ]\n",
      " [181.68842 ]\n",
      " [197.93866 ]\n",
      " [146.49577 ]\n",
      " [109.95069 ]\n",
      " [146.64331 ]\n",
      " [106.137375]\n",
      " [178.98183 ]\n",
      " [167.26257 ]]\n",
      "1620 Cost:  23.34762 \n",
      "Prediction:\n",
      " [[150.35721 ]\n",
      " [188.87527 ]\n",
      " [181.68906 ]\n",
      " [197.94374 ]\n",
      " [146.46571 ]\n",
      " [109.93036 ]\n",
      " [146.6654  ]\n",
      " [106.180595]\n",
      " [178.95782 ]\n",
      " [167.24625 ]]\n",
      "1640 Cost:  23.149982 \n",
      "Prediction:\n",
      " [[150.37463]\n",
      " [188.8544 ]\n",
      " [181.6897 ]\n",
      " [197.94878]\n",
      " [146.43579]\n",
      " [109.91013]\n",
      " [146.6874 ]\n",
      " [106.2236 ]\n",
      " [178.93393]\n",
      " [167.23001]]\n",
      "1660 Cost:  22.954485 \n",
      "Prediction:\n",
      " [[150.39198]\n",
      " [188.83365]\n",
      " [181.69032]\n",
      " [197.95381]\n",
      " [146.40605]\n",
      " [109.89004]\n",
      " [146.70929]\n",
      " [106.26641]\n",
      " [178.91019]\n",
      " [167.2139 ]]\n",
      "1680 Cost:  22.761066 \n",
      "Prediction:\n",
      " [[150.40923]\n",
      " [188.81302]\n",
      " [181.69095]\n",
      " [197.95882]\n",
      " [146.37645]\n",
      " [109.87005]\n",
      " [146.73108]\n",
      " [106.309  ]\n",
      " [178.8866 ]\n",
      " [167.19789]]\n",
      "1700 Cost:  22.569675 \n",
      "Prediction:\n",
      " [[150.42638]\n",
      " [188.79248]\n",
      " [181.69156]\n",
      " [197.9638 ]\n",
      " [146.34702]\n",
      " [109.85018]\n",
      " [146.75275]\n",
      " [106.35139]\n",
      " [178.86313]\n",
      " [167.18198]]\n",
      "1720 Cost:  22.380249 \n",
      "Prediction:\n",
      " [[150.44342]\n",
      " [188.77206]\n",
      " [181.69217]\n",
      " [197.96875]\n",
      " [146.31772]\n",
      " [109.83041]\n",
      " [146.77432]\n",
      " [106.39358]\n",
      " [178.83978]\n",
      " [167.1662 ]]\n",
      "1740 Cost:  22.192833 \n",
      "Prediction:\n",
      " [[150.46037 ]\n",
      " [188.75174 ]\n",
      " [181.69278 ]\n",
      " [197.97366 ]\n",
      " [146.28857 ]\n",
      " [109.81076 ]\n",
      " [146.79579 ]\n",
      " [106.435555]\n",
      " [178.81656 ]\n",
      " [167.15051 ]]\n",
      "1760 Cost:  22.007406 \n",
      "Prediction:\n",
      " [[150.47723 ]\n",
      " [188.73152 ]\n",
      " [181.69337 ]\n",
      " [197.97858 ]\n",
      " [146.25958 ]\n",
      " [109.79122 ]\n",
      " [146.81717 ]\n",
      " [106.477325]\n",
      " [178.79349 ]\n",
      " [167.13493 ]]\n",
      "1780 Cost:  21.823864 \n",
      "Prediction:\n",
      " [[150.49399]\n",
      " [188.7114 ]\n",
      " [181.69394]\n",
      " [197.98346]\n",
      " [146.23071]\n",
      " [109.77178]\n",
      " [146.83841]\n",
      " [106.5189 ]\n",
      " [178.77054]\n",
      " [167.11945]]\n",
      "1800 Cost:  21.642319 \n",
      "Prediction:\n",
      " [[150.51067 ]\n",
      " [188.6914  ]\n",
      " [181.69455 ]\n",
      " [197.98831 ]\n",
      " [146.20203 ]\n",
      " [109.752464]\n",
      " [146.85959 ]\n",
      " [106.560265]\n",
      " [178.74771 ]\n",
      " [167.10406 ]]\n",
      "1820 Cost:  21.462627 \n",
      "Prediction:\n",
      " [[150.52722 ]\n",
      " [188.67151 ]\n",
      " [181.6951  ]\n",
      " [197.99313 ]\n",
      " [146.17346 ]\n",
      " [109.733246]\n",
      " [146.88065 ]\n",
      " [106.60143 ]\n",
      " [178.725   ]\n",
      " [167.08879 ]]\n",
      "1840 Cost:  21.2849 \n",
      "Prediction:\n",
      " [[150.54373]\n",
      " [188.65172]\n",
      " [181.69568]\n",
      " [197.99796]\n",
      " [146.14505]\n",
      " [109.71415]\n",
      " [146.9016 ]\n",
      " [106.6424 ]\n",
      " [178.70247]\n",
      " [167.07364]]\n",
      "1860 Cost:  21.10901 \n",
      "Prediction:\n",
      " [[150.56012 ]\n",
      " [188.63203 ]\n",
      " [181.69624 ]\n",
      " [198.00275 ]\n",
      " [146.1168  ]\n",
      " [109.69515 ]\n",
      " [146.92245 ]\n",
      " [106.683174]\n",
      " [178.68002 ]\n",
      " [167.05856 ]]\n",
      "1880 Cost:  20.934938 \n",
      "Prediction:\n",
      " [[150.57642 ]\n",
      " [188.61243 ]\n",
      " [181.69678 ]\n",
      " [198.0075  ]\n",
      " [146.08868 ]\n",
      " [109.676254]\n",
      " [146.94319 ]\n",
      " [106.72374 ]\n",
      " [178.65771 ]\n",
      " [167.0436  ]]\n",
      "1900 Cost:  20.762732 \n",
      "Prediction:\n",
      " [[150.59262 ]\n",
      " [188.59296 ]\n",
      " [181.69733 ]\n",
      " [198.01224 ]\n",
      " [146.06071 ]\n",
      " [109.65747 ]\n",
      " [146.96384 ]\n",
      " [106.764114]\n",
      " [178.63553 ]\n",
      " [167.02873 ]]\n",
      "1920 Cost:  20.59231 \n",
      "Prediction:\n",
      " [[150.60876 ]\n",
      " [188.57358 ]\n",
      " [181.69788 ]\n",
      " [198.01697 ]\n",
      " [146.03288 ]\n",
      " [109.638794]\n",
      " [146.9844  ]\n",
      " [106.8043  ]\n",
      " [178.61346 ]\n",
      " [167.01396 ]]\n",
      "1940 Cost:  20.423708 \n",
      "Prediction:\n",
      " [[150.62479 ]\n",
      " [188.55429 ]\n",
      " [181.6984  ]\n",
      " [198.02165 ]\n",
      " [146.00519 ]\n",
      " [109.620224]\n",
      " [147.00485 ]\n",
      " [106.84428 ]\n",
      " [178.59155 ]\n",
      " [166.9993  ]]\n",
      "1960 Cost:  20.256834 \n",
      "Prediction:\n",
      " [[150.64072 ]\n",
      " [188.5351  ]\n",
      " [181.69891 ]\n",
      " [198.02632 ]\n",
      " [145.97765 ]\n",
      " [109.601746]\n",
      " [147.0252  ]\n",
      " [106.88406 ]\n",
      " [178.56973 ]\n",
      " [166.98473 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980 Cost:  20.091806 \n",
      "Prediction:\n",
      " [[150.65659]\n",
      " [188.51605]\n",
      " [181.69946]\n",
      " [198.03098]\n",
      " [145.95026]\n",
      " [109.58339]\n",
      " [147.04547]\n",
      " [106.92366]\n",
      " [178.54805]\n",
      " [166.97026]]\n",
      "2000 Cost:  19.928453 \n",
      "Prediction:\n",
      " [[150.67236]\n",
      " [188.49707]\n",
      " [181.69998]\n",
      " [198.03561]\n",
      " [145.923  ]\n",
      " [109.56513]\n",
      " [147.06563]\n",
      " [106.96307]\n",
      " [178.52649]\n",
      " [166.95587]]\n"
     ]
    }
   ],
   "source": [
    "# start populating the filename queue\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], \\\n",
    "                                  feed_dict = {X: x_batch, Y: y_batch})\n",
    "    if step % 20 == 0:\n",
    "        print(step, 'Cost: ', cost_val, '\\nPrediction:\\n', hy_val)\n",
    "        \n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1, bias = True)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "Cost:  tensor(13100.7168) \n",
      "Prediction: \n",
      " tensor([[51.7628],\n",
      "        [61.3103],\n",
      "        [60.8637],\n",
      "        [65.8595],\n",
      "        [46.9899]], grad_fn=<ThAddmmBackward>)\n",
      "20 \n",
      "Cost:  tensor(1.9804) \n",
      "Prediction: \n",
      " tensor([[153.3028],\n",
      "        [183.3734],\n",
      "        [181.1243],\n",
      "        [196.8213],\n",
      "        [140.0973]], grad_fn=<ThAddmmBackward>)\n",
      "40 \n",
      "Cost:  tensor(1.9611) \n",
      "Prediction: \n",
      " tensor([[153.2934],\n",
      "        [183.3816],\n",
      "        [181.1223],\n",
      "        [196.8200],\n",
      "        [140.1076]], grad_fn=<ThAddmmBackward>)\n",
      "60 \n",
      "Cost:  tensor(1.9420) \n",
      "Prediction: \n",
      " tensor([[153.2832],\n",
      "        [183.3886],\n",
      "        [181.1192],\n",
      "        [196.8176],\n",
      "        [140.1170]], grad_fn=<ThAddmmBackward>)\n",
      "80 \n",
      "Cost:  tensor(1.9231) \n",
      "Prediction: \n",
      " tensor([[153.2730],\n",
      "        [183.3956],\n",
      "        [181.1161],\n",
      "        [196.8152],\n",
      "        [140.1264]], grad_fn=<ThAddmmBackward>)\n",
      "2000 \n",
      "Cost:  tensor(0.7990) \n",
      "Prediction: \n",
      " tensor([[152.5138],\n",
      "        [183.9178],\n",
      "        [180.8857],\n",
      "        [196.6312],\n",
      "        [140.8264]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor([[ 73.,  80.,  75.],\n",
    "       [ 93.,  88.,  93.],\n",
    "       [ 89.,  91.,  90.],\n",
    "       [ 96.,  98., 100.],\n",
    "       [ 73.,  66.,  70.]])\n",
    "y_data = torch.tensor([[152.], [185.], [180.], [196.], [142.]])\n",
    "\n",
    "for epoch in range(2001):\n",
    "    inputs = Variable(x_data)\n",
    "    target = Variable(y_data)\n",
    "    out = model(inputs)\n",
    "    loss = criterion(out, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch in [0, 20, 40, 60, 80, 2000]:\n",
    "        print(epoch, '\\nCost: ', loss.data[0], '\\nPrediction: \\n', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from files - manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(38180.5000)\n",
      "10 tensor(29.5627)\n",
      "20 tensor(27.6072)\n",
      "30 tensor(27.4736)\n",
      "40 tensor(27.3410)\n",
      "50 tensor(27.2093)\n",
      "60 tensor(27.0785)\n",
      "70 tensor(26.9485)\n",
      "80 tensor(26.8195)\n",
      "90 tensor(26.6913)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-01-test-score.csv', delimiter = ',', dtype = np.float32)\n",
    "x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # forward pass: compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch, loss.data[0])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from files - with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training cycle  \n",
    "for epoch in range(training_epochs):  \n",
    "    # loop over all batches  \n",
    "    for i in range(total_batch):  \n",
    "        batch_xs, batch_ys = _, _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the neural network terminology:\n",
    "- one **epoch** = one forward pass and one backward pass of *all* the training exmaples\n",
    "- **batch size** = # of training examples in the one forward/backward pass. The higher the batch size, the more memory space you'll need\n",
    "- \\# of **iterations** = # of passes, each pass using [batch size] # of examples. To be clear, one pass = one fw pass + one bw pass\n",
    "\n",
    "Example: with 1000 training examples and with the batch size of 500, you need 2 iterations to finish 1 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreDataset(Dataset):\n",
    "    ''' Score dataset'''\n",
    "    \n",
    "    # initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        # download, read data, etc.\n",
    "        xy = np.loadtxt('data-01-test-score.csv', delimiter = ',', dtype = np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # return one item on the index\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ScoreDataset()\n",
    "train_loader = DataLoader(dataset = dataset, batch_size = 10, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 \n",
      "Cost:  tensor(33259.2578) \n",
      "Prediction: \n",
      " tensor([[-17.8671],\n",
      "        [-28.8485],\n",
      "        [-24.5735],\n",
      "        [-27.6166],\n",
      "        [-23.0884],\n",
      "        [-19.1195],\n",
      "        [-19.8090],\n",
      "        [-13.2798],\n",
      "        [-29.6720],\n",
      "        [-29.9746]], grad_fn=<ThAddmmBackward>)\n",
      "0 1 \n",
      "Cost:  tensor(13573.4697) \n",
      "Prediction: \n",
      " tensor([[40.0831],\n",
      "        [36.0037],\n",
      "        [54.8527],\n",
      "        [46.3095],\n",
      "        [41.9919],\n",
      "        [49.9399],\n",
      "        [42.8509],\n",
      "        [52.6997],\n",
      "        [53.5528],\n",
      "        [48.5756]], grad_fn=<ThAddmmBackward>)\n",
      "0 2 \n",
      "Cost:  tensor(5929.4912) \n",
      "Prediction: \n",
      " tensor([[ 98.3548],\n",
      "        [ 94.4966],\n",
      "        [ 94.6363],\n",
      "        [ 92.6496],\n",
      "        [106.2801]], grad_fn=<ThAddmmBackward>)\n",
      "1 0 \n",
      "Cost:  tensor(1652.2717) \n",
      "Prediction: \n",
      " tensor([[120.0126],\n",
      "        [136.9013],\n",
      "        [138.7258],\n",
      "        [150.2372],\n",
      "        [103.3234],\n",
      "        [ 74.0956],\n",
      "        [113.3247],\n",
      "        [ 85.4221],\n",
      "        [125.2505],\n",
      "        [113.5429]], grad_fn=<ThAddmmBackward>)\n",
      "1 1 \n",
      "Cost:  tensor(560.6420) \n",
      "Prediction: \n",
      " tensor([[121.2501],\n",
      "        [116.0269],\n",
      "        [161.6306],\n",
      "        [135.0777],\n",
      "        [126.6236],\n",
      "        [156.3663],\n",
      "        [127.3904],\n",
      "        [153.8064],\n",
      "        [155.3535],\n",
      "        [139.6641]], grad_fn=<ThAddmmBackward>)\n",
      "1 2 \n",
      "Cost:  tensor(349.7495) \n",
      "Prediction: \n",
      " tensor([[158.1382],\n",
      "        [154.0386],\n",
      "        [151.6339],\n",
      "        [145.8786],\n",
      "        [172.0531]], grad_fn=<ThAddmmBackward>)\n",
      "2 0 \n",
      "Cost:  tensor(117.7385) \n",
      "Prediction: \n",
      " tensor([[149.8039],\n",
      "        [172.7224],\n",
      "        [174.0129],\n",
      "        [188.6718],\n",
      "        [130.6431],\n",
      "        [ 94.2456],\n",
      "        [142.0966],\n",
      "        [106.7544],\n",
      "        [158.7366],\n",
      "        [144.5699]], grad_fn=<ThAddmmBackward>)\n",
      "2 1 \n",
      "Cost:  tensor(29.4422) \n",
      "Prediction: \n",
      " tensor([[138.7909],\n",
      "        [133.3251],\n",
      "        [184.6999],\n",
      "        [154.2532],\n",
      "        [144.9147],\n",
      "        [179.3681],\n",
      "        [145.6520],\n",
      "        [175.6561],\n",
      "        [177.3459],\n",
      "        [159.3419]], grad_fn=<ThAddmmBackward>)\n",
      "2 2 \n",
      "Cost:  tensor(65.1507) \n",
      "Prediction: \n",
      " tensor([[171.0595],\n",
      "        [166.9097],\n",
      "        [163.9517],\n",
      "        [157.3711],\n",
      "        [186.2667]], grad_fn=<ThAddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    for i, data in enumerate(train_loader, 0): # train_loader -> iterable\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "    \n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "    \n",
    "        # run your training process\n",
    "        #print(epoch, i, 'inputs', inputs.data, 'labels', labels.data)\n",
    "    \n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(epoch, i, '\\nCost: ', loss.data[0], '\\nPrediction: \\n', y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
