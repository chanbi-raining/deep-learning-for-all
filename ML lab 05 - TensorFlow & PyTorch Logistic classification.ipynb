{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classification\n",
    "\n",
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "# look out for the shape\n",
    "X = tf.placeholder(tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "W = tf.Variable(tf.random_normal([2, 1], name = 'weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name = 'bias'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "# sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.6283978\n",
      "200 0.5807934\n",
      "400 0.476045\n",
      "600 0.42803445\n",
      "800 0.4004855\n",
      "1000 0.38139525\n",
      "1200 0.3663261\n",
      "1400 0.3534359\n",
      "1600 0.3418838\n",
      "1800 0.3312547\n",
      "2000 0.32132834\n",
      "2200 0.31197944\n",
      "2400 0.30313045\n",
      "2600 0.29472947\n",
      "2800 0.28673837\n",
      "3000 0.279127\n",
      "3200 0.2718701\n",
      "3400 0.2649457\n",
      "3600 0.25833377\n",
      "3800 0.25201604\n",
      "4000 0.24597573\n",
      "4200 0.24019705\n",
      "4400 0.23466533\n",
      "4600 0.22936672\n",
      "4800 0.2242883\n",
      "5000 0.21941799\n",
      "5200 0.21474445\n",
      "5400 0.21025698\n",
      "5600 0.20594566\n",
      "5800 0.20180102\n",
      "6000 0.19781435\n",
      "6200 0.19397743\n",
      "6400 0.19028254\n",
      "6600 0.18672244\n",
      "6800 0.18329032\n",
      "7000 0.17997967\n",
      "7200 0.17678457\n",
      "7400 0.17369942\n",
      "7600 0.17071891\n",
      "7800 0.1678379\n",
      "8000 0.16505183\n",
      "8200 0.16235629\n",
      "8400 0.15974697\n",
      "8600 0.15722004\n",
      "8800 0.15477178\n",
      "9000 0.1523986\n",
      "9200 0.1500973\n",
      "9400 0.14786476\n",
      "9600 0.14569798\n",
      "9800 0.14359415\n",
      "10000 0.14155073\n",
      "\n",
      "Hypothesis:  [[0.02747056]\n",
      " [0.15416528]\n",
      " [0.28881577]\n",
      " [0.7888136 ]\n",
      " [0.9441614 ]\n",
      " [0.9817187 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict = {X: x_data, Y: y_data})\n",
    "    print('\\nHypothesis: ', h, '\\nCorrect (Y): ', c, '\\nAccuracy: ', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter = ',', dtype = np.float32)\n",
    "x_data = xy[:, :-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.0526137\n",
      "200 0.9588531\n",
      "400 0.693748\n",
      "600 0.64048344\n",
      "800 0.6218877\n",
      "1000 0.60999364\n",
      "1200 0.6000279\n",
      "1400 0.5910655\n",
      "1600 0.5828812\n",
      "1800 0.5753848\n",
      "2000 0.5685144\n",
      "2200 0.5622161\n",
      "2400 0.5564403\n",
      "2600 0.55114084\n",
      "2800 0.5462752\n",
      "3000 0.5418042\n",
      "3200 0.53769207\n",
      "3400 0.53390616\n",
      "3600 0.53041655\n",
      "3800 0.52719665\n",
      "4000 0.5242216\n",
      "4200 0.5214697\n",
      "4400 0.51892084\n",
      "4600 0.51655704\n",
      "4800 0.51436204\n",
      "5000 0.51232123\n",
      "5200 0.51042134\n",
      "5400 0.50865036\n",
      "5600 0.5069976\n",
      "5800 0.50545305\n",
      "6000 0.504008\n",
      "6200 0.5026544\n",
      "6400 0.5013849\n",
      "6600 0.5001929\n",
      "6800 0.4990724\n",
      "7000 0.49801788\n",
      "7200 0.49702448\n",
      "7400 0.49608752\n",
      "7600 0.49520287\n",
      "7800 0.4943668\n",
      "8000 0.49357572\n",
      "8200 0.49282652\n",
      "8400 0.49211642\n",
      "8600 0.49144253\n",
      "8800 0.4908025\n",
      "9000 0.49019417\n",
      "9200 0.48961517\n",
      "9400 0.4890639\n",
      "9600 0.48853832\n",
      "9800 0.4880371\n",
      "10000 0.4875585\n",
      "\n",
      "Hypothesis:  [[0.41553873]\n",
      " [0.918975  ]\n",
      " [0.16440071]\n",
      " [0.9478924 ]\n",
      " [0.24734591]\n",
      " [0.7051123 ]\n",
      " [0.9492146 ]\n",
      " [0.62344855]\n",
      " [0.20957765]\n",
      " [0.50526565]\n",
      " [0.6855548 ]\n",
      " [0.17293864]\n",
      " [0.22321092]\n",
      " [0.26375306]\n",
      " [0.73591596]\n",
      " [0.5176815 ]\n",
      " [0.7048676 ]\n",
      " [0.91022223]\n",
      " [0.82458323]\n",
      " [0.5810273 ]\n",
      " [0.68212557]\n",
      " [0.09449586]\n",
      " [0.62722117]\n",
      " [0.70127463]\n",
      " [0.3801746 ]\n",
      " [0.92594093]\n",
      " [0.5338927 ]\n",
      " [0.607921  ]\n",
      " [0.74588555]\n",
      " [0.3924362 ]\n",
      " [0.9528994 ]\n",
      " [0.80670506]\n",
      " [0.57079947]\n",
      " [0.80714107]\n",
      " [0.3545689 ]\n",
      " [0.6661543 ]\n",
      " [0.8391796 ]\n",
      " [0.5811844 ]\n",
      " [0.44844362]\n",
      " [0.37513512]\n",
      " [0.80137986]\n",
      " [0.17657876]\n",
      " [0.35963944]\n",
      " [0.06417592]\n",
      " [0.533988  ]\n",
      " [0.9268445 ]\n",
      " [0.7574769 ]\n",
      " [0.72013724]\n",
      " [0.91615176]\n",
      " [0.93282926]\n",
      " [0.93004483]\n",
      " [0.22293192]\n",
      " [0.36366287]\n",
      " [0.96859723]\n",
      " [0.21581486]\n",
      " [0.5367115 ]\n",
      " [0.13629283]\n",
      " [0.7677386 ]\n",
      " [0.8853393 ]\n",
      " [0.49309054]\n",
      " [0.94649446]\n",
      " [0.66799366]\n",
      " [0.66882235]\n",
      " [0.845884  ]\n",
      " [0.5905184 ]\n",
      " [0.605441  ]\n",
      " [0.94812155]\n",
      " [0.64707154]\n",
      " [0.86782783]\n",
      " [0.6444268 ]\n",
      " [0.30561477]\n",
      " [0.7129874 ]\n",
      " [0.91448903]\n",
      " [0.92394966]\n",
      " [0.8856161 ]\n",
      " [0.8128561 ]\n",
      " [0.47881123]\n",
      " [0.84497947]\n",
      " [0.892166  ]\n",
      " [0.9209399 ]\n",
      " [0.87215817]\n",
      " [0.80917764]\n",
      " [0.38538277]\n",
      " [0.8108541 ]\n",
      " [0.5768447 ]\n",
      " [0.8938647 ]\n",
      " [0.48874202]\n",
      " [0.88570464]\n",
      " [0.92895186]\n",
      " [0.773965  ]\n",
      " [0.8702311 ]\n",
      " [0.61117357]\n",
      " [0.6836561 ]\n",
      " [0.5944889 ]\n",
      " [0.90646994]\n",
      " [0.97522527]\n",
      " [0.9027652 ]\n",
      " [0.6422892 ]\n",
      " [0.23436718]\n",
      " [0.5723836 ]\n",
      " [0.53102845]\n",
      " [0.9573479 ]\n",
      " [0.7947307 ]\n",
      " [0.77280897]\n",
      " [0.799268  ]\n",
      " [0.6907101 ]\n",
      " [0.9350195 ]\n",
      " [0.8422045 ]\n",
      " [0.4624192 ]\n",
      " [0.37840798]\n",
      " [0.92871624]\n",
      " [0.87961566]\n",
      " [0.43991494]\n",
      " [0.39571533]\n",
      " [0.6218165 ]\n",
      " [0.8558608 ]\n",
      " [0.8653498 ]\n",
      " [0.921181  ]\n",
      " [0.18614666]\n",
      " [0.73461044]\n",
      " [0.85305434]\n",
      " [0.59567285]\n",
      " [0.61156315]\n",
      " [0.9042935 ]\n",
      " [0.7528429 ]\n",
      " [0.8445825 ]\n",
      " [0.8133259 ]\n",
      " [0.5994814 ]\n",
      " [0.4871793 ]\n",
      " [0.42616683]\n",
      " [0.41750267]\n",
      " [0.8177417 ]\n",
      " [0.92373747]\n",
      " [0.8499384 ]\n",
      " [0.8198375 ]\n",
      " [0.86218596]\n",
      " [0.41227943]\n",
      " [0.8196937 ]\n",
      " [0.6681532 ]\n",
      " [0.7584512 ]\n",
      " [0.89393204]\n",
      " [0.6513024 ]\n",
      " [0.55697596]\n",
      " [0.73969287]\n",
      " [0.9199064 ]\n",
      " [0.74569803]\n",
      " [0.45418903]\n",
      " [0.9264915 ]\n",
      " [0.6174861 ]\n",
      " [0.72996145]\n",
      " [0.26111475]\n",
      " [0.3666547 ]\n",
      " [0.13399912]\n",
      " [0.28778177]\n",
      " [0.92231387]\n",
      " [0.8664782 ]\n",
      " [0.9408905 ]\n",
      " [0.14571066]\n",
      " [0.45260146]\n",
      " [0.8100191 ]\n",
      " [0.6693816 ]\n",
      " [0.87887233]\n",
      " [0.37151295]\n",
      " [0.81608826]\n",
      " [0.598138  ]\n",
      " [0.60900164]\n",
      " [0.7118624 ]\n",
      " [0.8503847 ]\n",
      " [0.7453363 ]\n",
      " [0.6470306 ]\n",
      " [0.88670903]\n",
      " [0.90966445]\n",
      " [0.9555833 ]\n",
      " [0.20854522]\n",
      " [0.8131214 ]\n",
      " [0.36426812]\n",
      " [0.43823534]\n",
      " [0.39741367]\n",
      " [0.8497034 ]\n",
      " [0.70725495]\n",
      " [0.926464  ]\n",
      " [0.90257144]\n",
      " [0.538895  ]\n",
      " [0.14227144]\n",
      " [0.18630095]\n",
      " [0.54142916]\n",
      " [0.7164379 ]\n",
      " [0.60039055]\n",
      " [0.8078273 ]\n",
      " [0.6360187 ]\n",
      " [0.32752073]\n",
      " [0.3149896 ]\n",
      " [0.90988445]\n",
      " [0.3782265 ]\n",
      " [0.85359746]\n",
      " [0.88909096]\n",
      " [0.7297382 ]\n",
      " [0.63804835]\n",
      " [0.642265  ]\n",
      " [0.55278546]\n",
      " [0.7073302 ]\n",
      " [0.9354376 ]\n",
      " [0.8150196 ]\n",
      " [0.77873284]\n",
      " [0.14453235]\n",
      " [0.28466812]\n",
      " [0.9224529 ]\n",
      " [0.1938016 ]\n",
      " [0.9345695 ]\n",
      " [0.27803624]\n",
      " [0.24043494]\n",
      " [0.54455125]\n",
      " [0.7211562 ]\n",
      " [0.25314477]\n",
      " [0.77445847]\n",
      " [0.7193961 ]\n",
      " [0.80400085]\n",
      " [0.7005075 ]\n",
      " [0.16444567]\n",
      " [0.3099045 ]\n",
      " [0.6794224 ]\n",
      " [0.5483191 ]\n",
      " [0.915586  ]\n",
      " [0.9395857 ]\n",
      " [0.6993222 ]\n",
      " [0.38236794]\n",
      " [0.03139599]\n",
      " [0.72028756]\n",
      " [0.356455  ]\n",
      " [0.53038114]\n",
      " [0.94061166]\n",
      " [0.63069975]\n",
      " [0.95087785]\n",
      " [0.23442015]\n",
      " [0.14384805]\n",
      " [0.24855152]\n",
      " [0.67448735]\n",
      " [0.9216785 ]\n",
      " [0.8872355 ]\n",
      " [0.5929708 ]\n",
      " [0.68022543]\n",
      " [0.5943589 ]\n",
      " [0.13287063]\n",
      " [0.55288994]\n",
      " [0.1602861 ]\n",
      " [0.6024153 ]\n",
      " [0.84680074]\n",
      " [0.6988087 ]\n",
      " [0.6293765 ]\n",
      " [0.9420738 ]\n",
      " [0.83592385]\n",
      " [0.80512875]\n",
      " [0.7824268 ]\n",
      " [0.76735485]\n",
      " [0.84900224]\n",
      " [0.3465411 ]\n",
      " [0.39309853]\n",
      " [0.47675005]\n",
      " [0.8152172 ]\n",
      " [0.6719812 ]\n",
      " [0.6795234 ]\n",
      " [0.83735824]\n",
      " [0.33082077]\n",
      " [0.51254684]\n",
      " [0.6078336 ]\n",
      " [0.57142305]\n",
      " [0.4897692 ]\n",
      " [0.9047396 ]\n",
      " [0.69785744]\n",
      " [0.934064  ]\n",
      " [0.5977118 ]\n",
      " [0.7977195 ]\n",
      " [0.81333894]\n",
      " [0.8246477 ]\n",
      " [0.63731784]\n",
      " [0.8518672 ]\n",
      " [0.35724035]\n",
      " [0.62173283]\n",
      " [0.6811482 ]\n",
      " [0.3125366 ]\n",
      " [0.78170705]\n",
      " [0.2769362 ]\n",
      " [0.6452415 ]\n",
      " [0.9321556 ]\n",
      " [0.800954  ]\n",
      " [0.8799593 ]\n",
      " [0.73781574]\n",
      " [0.5273429 ]\n",
      " [0.7009994 ]\n",
      " [0.38488808]\n",
      " [0.48292133]\n",
      " [0.5921825 ]\n",
      " [0.5970675 ]\n",
      " [0.70959115]\n",
      " [0.5834428 ]\n",
      " [0.19273402]\n",
      " [0.67653906]\n",
      " [0.91566354]\n",
      " [0.556719  ]\n",
      " [0.51949537]\n",
      " [0.7838695 ]\n",
      " [0.45659503]\n",
      " [0.724402  ]\n",
      " [0.50458187]\n",
      " [0.70807904]\n",
      " [0.89945143]\n",
      " [0.70334095]\n",
      " [0.6520909 ]\n",
      " [0.87759066]\n",
      " [0.573926  ]\n",
      " [0.866672  ]\n",
      " [0.9266586 ]\n",
      " [0.24339052]\n",
      " [0.81105596]\n",
      " [0.1882189 ]\n",
      " [0.77547646]\n",
      " [0.799639  ]\n",
      " [0.6902929 ]\n",
      " [0.31575716]\n",
      " [0.8086722 ]\n",
      " [0.686474  ]\n",
      " [0.7618356 ]\n",
      " [0.17599319]\n",
      " [0.84554774]\n",
      " [0.85886294]\n",
      " [0.5383398 ]\n",
      " [0.9526054 ]\n",
      " [0.2869698 ]\n",
      " [0.65135187]\n",
      " [0.94512445]\n",
      " [0.26159742]\n",
      " [0.49940893]\n",
      " [0.6695623 ]\n",
      " [0.32307237]\n",
      " [0.17249526]\n",
      " [0.82245857]\n",
      " [0.9099874 ]\n",
      " [0.85130334]\n",
      " [0.5905348 ]\n",
      " [0.7140784 ]\n",
      " [0.6088927 ]\n",
      " [0.774857  ]\n",
      " [0.76872855]\n",
      " [0.92612493]\n",
      " [0.7780507 ]\n",
      " [0.7906079 ]\n",
      " [0.5247727 ]\n",
      " [0.94406897]\n",
      " [0.9423456 ]\n",
      " [0.7807288 ]\n",
      " [0.23132427]\n",
      " [0.7070807 ]\n",
      " [0.4414293 ]\n",
      " [0.8022113 ]\n",
      " [0.18113868]\n",
      " [0.22481349]\n",
      " [0.40868524]\n",
      " [0.71038663]\n",
      " [0.4070049 ]\n",
      " [0.5681945 ]\n",
      " [0.87250304]\n",
      " [0.5994474 ]\n",
      " [0.8491254 ]\n",
      " [0.94132537]\n",
      " [0.7228917 ]\n",
      " [0.0768219 ]\n",
      " [0.5289679 ]\n",
      " [0.8876022 ]\n",
      " [0.8742441 ]\n",
      " [0.7099343 ]\n",
      " [0.32413793]\n",
      " [0.8673007 ]\n",
      " [0.91313195]\n",
      " [0.3455731 ]\n",
      " [0.65478146]\n",
      " [0.83240336]\n",
      " [0.81262475]\n",
      " [0.8754486 ]\n",
      " [0.89819413]\n",
      " [0.84837997]\n",
      " [0.91257745]\n",
      " [0.6804287 ]\n",
      " [0.6531473 ]\n",
      " [0.5577279 ]\n",
      " [0.8447579 ]\n",
      " [0.88832384]\n",
      " [0.25345913]\n",
      " [0.7972552 ]\n",
      " [0.85895705]\n",
      " [0.28345147]\n",
      " [0.5978835 ]\n",
      " [0.8635544 ]\n",
      " [0.55332154]\n",
      " [0.8827686 ]\n",
      " [0.24696474]\n",
      " [0.8345219 ]\n",
      " [0.6117849 ]\n",
      " [0.88880986]\n",
      " [0.3453895 ]\n",
      " [0.77083313]\n",
      " [0.69925576]\n",
      " [0.7482465 ]\n",
      " [0.07054002]\n",
      " [0.22692628]\n",
      " [0.66694516]\n",
      " [0.8216911 ]\n",
      " [0.47435266]\n",
      " [0.77033377]\n",
      " [0.5473267 ]\n",
      " [0.31703115]\n",
      " [0.8288007 ]\n",
      " [0.46474433]\n",
      " [0.88530123]\n",
      " [0.7984882 ]\n",
      " [0.6967052 ]\n",
      " [0.9222951 ]\n",
      " [0.74633455]\n",
      " [0.8158898 ]\n",
      " [0.38438672]\n",
      " [0.2632716 ]\n",
      " [0.76650697]\n",
      " [0.427708  ]\n",
      " [0.46203807]\n",
      " [0.9062363 ]\n",
      " [0.8674798 ]\n",
      " [0.9185021 ]\n",
      " [0.95118964]\n",
      " [0.6104351 ]\n",
      " [0.8843028 ]\n",
      " [0.42128962]\n",
      " [0.3825015 ]\n",
      " [0.420109  ]\n",
      " [0.9297395 ]\n",
      " [0.58335805]\n",
      " [0.14061219]\n",
      " [0.93549824]\n",
      " [0.8262076 ]\n",
      " [0.55935794]\n",
      " [0.81145173]\n",
      " [0.03068982]\n",
      " [0.9123532 ]\n",
      " [0.7838232 ]\n",
      " [0.7746256 ]\n",
      " [0.76862437]\n",
      " [0.95951295]\n",
      " [0.57324094]\n",
      " [0.8210349 ]\n",
      " [0.69715106]\n",
      " [0.8865145 ]\n",
      " [0.20657806]\n",
      " [0.62036586]\n",
      " [0.91249424]\n",
      " [0.62654036]\n",
      " [0.72278035]\n",
      " [0.9287052 ]\n",
      " [0.87380695]\n",
      " [0.8734606 ]\n",
      " [0.40996104]\n",
      " [0.7929492 ]\n",
      " [0.9529359 ]\n",
      " [0.78729254]\n",
      " [0.6417138 ]\n",
      " [0.35998592]\n",
      " [0.47022632]\n",
      " [0.5162504 ]\n",
      " [0.6614601 ]\n",
      " [0.4754816 ]\n",
      " [0.7523955 ]\n",
      " [0.5533568 ]\n",
      " [0.7661978 ]\n",
      " [0.8066652 ]\n",
      " [0.7127398 ]\n",
      " [0.6245404 ]\n",
      " [0.4948027 ]\n",
      " [0.57071877]\n",
      " [0.9409495 ]\n",
      " [0.8629569 ]\n",
      " [0.31138954]\n",
      " [0.4917054 ]\n",
      " [0.56297755]\n",
      " [0.14759971]\n",
      " [0.8752534 ]\n",
      " [0.1184747 ]\n",
      " [0.9178262 ]\n",
      " [0.8729055 ]\n",
      " [0.8390967 ]\n",
      " [0.6891496 ]\n",
      " [0.89417434]\n",
      " [0.31991386]\n",
      " [0.7376691 ]\n",
      " [0.9415693 ]\n",
      " [0.26998952]\n",
      " [0.40907118]\n",
      " [0.85332614]\n",
      " [0.88843364]\n",
      " [0.7068434 ]\n",
      " [0.8458835 ]\n",
      " [0.82919383]\n",
      " [0.8094614 ]\n",
      " [0.26444924]\n",
      " [0.7740875 ]\n",
      " [0.9229144 ]\n",
      " [0.58920234]\n",
      " [0.8104929 ]\n",
      " [0.6992631 ]\n",
      " [0.8040278 ]\n",
      " [0.8459566 ]\n",
      " [0.9327022 ]\n",
      " [0.5909448 ]\n",
      " [0.39351454]\n",
      " [0.8108583 ]\n",
      " [0.7084784 ]\n",
      " [0.963745  ]\n",
      " [0.75242937]\n",
      " [0.70112747]\n",
      " [0.42297778]\n",
      " [0.7031312 ]\n",
      " [0.92236143]\n",
      " [0.9432889 ]\n",
      " [0.88011974]\n",
      " [0.70091057]\n",
      " [0.6163809 ]\n",
      " [0.8102233 ]\n",
      " [0.5323772 ]\n",
      " [0.84756887]\n",
      " [0.7971201 ]\n",
      " [0.9100884 ]\n",
      " [0.5982424 ]\n",
      " [0.6715922 ]\n",
      " [0.8995391 ]\n",
      " [0.49120367]\n",
      " [0.49917987]\n",
      " [0.71460456]\n",
      " [0.72388947]\n",
      " [0.6945463 ]\n",
      " [0.9158015 ]\n",
      " [0.9228755 ]\n",
      " [0.18241076]\n",
      " [0.17064908]\n",
      " [0.77075887]\n",
      " [0.49394658]\n",
      " [0.15534821]\n",
      " [0.8330976 ]\n",
      " [0.90909964]\n",
      " [0.6676487 ]\n",
      " [0.93899024]\n",
      " [0.93320876]\n",
      " [0.737633  ]\n",
      " [0.85420156]\n",
      " [0.6816047 ]\n",
      " [0.67358357]\n",
      " [0.76066566]\n",
      " [0.61512995]\n",
      " [0.13562463]\n",
      " [0.91701734]\n",
      " [0.8884433 ]\n",
      " [0.6995874 ]\n",
      " [0.9198951 ]\n",
      " [0.8955757 ]\n",
      " [0.90373194]\n",
      " [0.57334334]\n",
      " [0.7231606 ]\n",
      " [0.8884642 ]\n",
      " [0.62663203]\n",
      " [0.8575428 ]\n",
      " [0.9242819 ]\n",
      " [0.51987123]\n",
      " [0.832627  ]\n",
      " [0.8416068 ]\n",
      " [0.595221  ]\n",
      " [0.48742774]\n",
      " [0.07794283]\n",
      " [0.2821516 ]\n",
      " [0.81863886]\n",
      " [0.6499241 ]\n",
      " [0.71121556]\n",
      " [0.5506632 ]\n",
      " [0.92968404]\n",
      " [0.45315015]\n",
      " [0.7752036 ]\n",
      " [0.2574611 ]\n",
      " [0.8753263 ]\n",
      " [0.4256748 ]\n",
      " [0.78347915]\n",
      " [0.5569667 ]\n",
      " [0.8719318 ]\n",
      " [0.583622  ]\n",
      " [0.210449  ]\n",
      " [0.85205287]\n",
      " [0.95891416]\n",
      " [0.35920712]\n",
      " [0.9228162 ]\n",
      " [0.8360393 ]\n",
      " [0.8250846 ]\n",
      " [0.77498937]\n",
      " [0.44857666]\n",
      " [0.27416724]\n",
      " [0.7361147 ]\n",
      " [0.16988526]\n",
      " [0.938203  ]\n",
      " [0.3627175 ]\n",
      " [0.91639173]\n",
      " [0.8928254 ]\n",
      " [0.43432707]\n",
      " [0.19677047]\n",
      " [0.68399817]\n",
      " [0.4926055 ]\n",
      " [0.7939121 ]\n",
      " [0.6287437 ]\n",
      " [0.9769046 ]\n",
      " [0.5364544 ]\n",
      " [0.63250387]\n",
      " [0.7834411 ]\n",
      " [0.76285434]\n",
      " [0.06792115]\n",
      " [0.83293337]\n",
      " [0.80996996]\n",
      " [0.84661853]\n",
      " [0.60057575]\n",
      " [0.4582778 ]\n",
      " [0.5936689 ]\n",
      " [0.9046062 ]\n",
      " [0.60480773]\n",
      " [0.78273845]\n",
      " [0.78279924]\n",
      " [0.8293736 ]\n",
      " [0.78772604]\n",
      " [0.5730983 ]\n",
      " [0.77706194]\n",
      " [0.90063065]\n",
      " [0.75569826]\n",
      " [0.9445586 ]\n",
      " [0.7855542 ]\n",
      " [0.60084385]\n",
      " [0.44645476]\n",
      " [0.8192356 ]\n",
      " [0.8329967 ]\n",
      " [0.487469  ]\n",
      " [0.5869256 ]\n",
      " [0.2711947 ]\n",
      " [0.53477365]\n",
      " [0.8048842 ]\n",
      " [0.9462188 ]\n",
      " [0.85124624]\n",
      " [0.7286561 ]\n",
      " [0.7271317 ]\n",
      " [0.891121  ]\n",
      " [0.5434939 ]\n",
      " [0.916949  ]\n",
      " [0.58618927]\n",
      " [0.84915495]\n",
      " [0.26655918]\n",
      " [0.08233364]\n",
      " [0.2475273 ]\n",
      " [0.3653953 ]\n",
      " [0.7275852 ]\n",
      " [0.8148103 ]\n",
      " [0.61197805]\n",
      " [0.7393623 ]\n",
      " [0.8468897 ]\n",
      " [0.49465296]\n",
      " [0.4002912 ]\n",
      " [0.90847564]\n",
      " [0.90119106]\n",
      " [0.49134865]\n",
      " [0.7098531 ]\n",
      " [0.15064344]\n",
      " [0.31862292]\n",
      " [0.7677424 ]\n",
      " [0.7295788 ]\n",
      " [0.8931816 ]\n",
      " [0.97727054]\n",
      " [0.24696599]\n",
      " [0.7971813 ]\n",
      " [0.5636613 ]\n",
      " [0.44722116]\n",
      " [0.714676  ]\n",
      " [0.6584962 ]\n",
      " [0.8994765 ]\n",
      " [0.6577808 ]\n",
      " [0.5496767 ]\n",
      " [0.58467054]\n",
      " [0.14880876]\n",
      " [0.70838964]\n",
      " [0.5796109 ]\n",
      " [0.8886926 ]\n",
      " [0.5319571 ]\n",
      " [0.5355667 ]\n",
      " [0.76319253]\n",
      " [0.72118914]\n",
      " [0.5211723 ]\n",
      " [0.75785834]\n",
      " [0.62213683]\n",
      " [0.3073976 ]\n",
      " [0.6449737 ]\n",
      " [0.8760572 ]\n",
      " [0.8612717 ]\n",
      " [0.6069431 ]\n",
      " [0.81754035]\n",
      " [0.2695278 ]\n",
      " [0.8702417 ]\n",
      " [0.58552676]\n",
      " [0.7479931 ]\n",
      " [0.49479395]\n",
      " [0.67243403]\n",
      " [0.8124723 ]\n",
      " [0.18200846]\n",
      " [0.26555356]\n",
      " [0.80602455]\n",
      " [0.8275786 ]\n",
      " [0.822802  ]\n",
      " [0.8995823 ]\n",
      " [0.8281088 ]\n",
      " [0.68327457]\n",
      " [0.75851357]\n",
      " [0.73607206]\n",
      " [0.7237254 ]\n",
      " [0.795642  ]\n",
      " [0.45350584]\n",
      " [0.2987245 ]\n",
      " [0.8835109 ]\n",
      " [0.77613086]\n",
      " [0.5690234 ]\n",
      " [0.29161522]\n",
      " [0.8808963 ]\n",
      " [0.77562815]\n",
      " [0.8571977 ]\n",
      " [0.6467124 ]\n",
      " [0.9017389 ]\n",
      " [0.904542  ]\n",
      " [0.7958321 ]\n",
      " [0.49669334]\n",
      " [0.91229606]\n",
      " [0.91247696]\n",
      " [0.2917614 ]\n",
      " [0.1775043 ]\n",
      " [0.7307401 ]\n",
      " [0.43114156]\n",
      " [0.8537608 ]\n",
      " [0.33402494]\n",
      " [0.39944983]\n",
      " [0.38456243]\n",
      " [0.78517395]\n",
      " [0.8567884 ]\n",
      " [0.13982506]\n",
      " [0.3667793 ]\n",
      " [0.59492457]\n",
      " [0.45525208]\n",
      " [0.5517027 ]\n",
      " [0.78599495]\n",
      " [0.145295  ]\n",
      " [0.91972667]\n",
      " [0.22961634]\n",
      " [0.8266743 ]\n",
      " [0.7537949 ]\n",
      " [0.7440125 ]\n",
      " [0.81123024]\n",
      " [0.7085487 ]\n",
      " [0.8880626 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.7628459\n"
     ]
    }
   ],
   "source": [
    "# look out for the shape\n",
    "X = tf.placeholder(tf.float32, shape = [None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "W = tf.Variable(tf.random_normal([8, 1], name = 'weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name = 'bias'))\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict = {X: x_data, Y: y_data})\n",
    "    print('\\nHypothesis: ', h, '\\nCorrect (Y): ', c, '\\nAccuracy: ', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticClassificationModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.Tensor([[1., 2.], [2., 3.], [3., 1.], [4., 3.], [5., 3.], [6., 2.]])\n",
    "y_data = torch.Tensor([[0], [0], [0], [1.], [1.], [1.]])\n",
    "\n",
    "# Hyper Parameters\n",
    "input_size = 2\n",
    "num_classes = 1\n",
    "num_epochs = 10001\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tCost:  tensor(0.5052)\n",
      "200 \tCost:  tensor(0.4784)\n",
      "400 \tCost:  tensor(0.4575)\n",
      "600 \tCost:  tensor(0.4396)\n",
      "800 \tCost:  tensor(0.4236)\n",
      "1000 \tCost:  tensor(0.4089)\n",
      "1200 \tCost:  tensor(0.3952)\n",
      "1400 \tCost:  tensor(0.3822)\n",
      "1600 \tCost:  tensor(0.3700)\n",
      "1800 \tCost:  tensor(0.3583)\n",
      "2000 \tCost:  tensor(0.3473)\n",
      "2200 \tCost:  tensor(0.3368)\n",
      "2400 \tCost:  tensor(0.3268)\n",
      "2600 \tCost:  tensor(0.3173)\n",
      "2800 \tCost:  tensor(0.3083)\n",
      "3000 \tCost:  tensor(0.2997)\n",
      "3200 \tCost:  tensor(0.2914)\n",
      "3400 \tCost:  tensor(0.2836)\n",
      "3600 \tCost:  tensor(0.2762)\n",
      "3800 \tCost:  tensor(0.2690)\n",
      "4000 \tCost:  tensor(0.2622)\n",
      "4200 \tCost:  tensor(0.2558)\n",
      "4400 \tCost:  tensor(0.2495)\n",
      "4600 \tCost:  tensor(0.2436)\n",
      "4800 \tCost:  tensor(0.2379)\n",
      "5000 \tCost:  tensor(0.2325)\n",
      "5200 \tCost:  tensor(0.2273)\n",
      "5400 \tCost:  tensor(0.2223)\n",
      "5600 \tCost:  tensor(0.2175)\n",
      "5800 \tCost:  tensor(0.2129)\n",
      "6000 \tCost:  tensor(0.2085)\n",
      "6200 \tCost:  tensor(0.2043)\n",
      "6400 \tCost:  tensor(0.2002)\n",
      "6600 \tCost:  tensor(0.1962)\n",
      "6800 \tCost:  tensor(0.1925)\n",
      "7000 \tCost:  tensor(0.1888)\n",
      "7200 \tCost:  tensor(0.1853)\n",
      "7400 \tCost:  tensor(0.1819)\n",
      "7600 \tCost:  tensor(0.1787)\n",
      "7800 \tCost:  tensor(0.1755)\n",
      "8000 \tCost:  tensor(0.1725)\n",
      "8200 \tCost:  tensor(0.1695)\n",
      "8400 \tCost:  tensor(0.1667)\n",
      "8600 \tCost:  tensor(0.1640)\n",
      "8800 \tCost:  tensor(0.1613)\n",
      "9000 \tCost:  tensor(0.1587)\n",
      "9200 \tCost:  tensor(0.1562)\n",
      "9400 \tCost:  tensor(0.1538)\n",
      "9600 \tCost:  tensor(0.1515)\n",
      "9800 \tCost:  tensor(0.1492)\n",
      "10000 \tCost:  tensor(0.1470)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticClassificationModel(input_size, num_classes)\n",
    "\n",
    "criterion = nn.BCELoss() # binary cost entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(epoch, '\\tCost: ', loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: 1 if x > 0.5 else 0, model(x_data).data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter = ',', dtype = np.float32)\n",
    "x_data = Variable(torch.Tensor(xy[:, :-1]))\n",
    "y_data = Variable(torch.Tensor(xy[:, [-1]]))\n",
    "\n",
    "# Hyper Parameters\n",
    "input_size = 8\n",
    "num_classes = 1\n",
    "num_epochs = 10001\n",
    "batch_size = 0\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tCost:  0.7403849363327026\n",
      "200 \tCost:  0.6635563373565674\n",
      "400 \tCost:  0.6389964818954468\n",
      "600 \tCost:  0.6224050521850586\n",
      "800 \tCost:  0.6084506511688232\n",
      "1000 \tCost:  0.596240222454071\n",
      "1200 \tCost:  0.5854698419570923\n",
      "1400 \tCost:  0.5759351253509521\n",
      "1600 \tCost:  0.5674687027931213\n",
      "1800 \tCost:  0.5599287748336792\n",
      "2000 \tCost:  0.5531917810440063\n",
      "2200 \tCost:  0.5471526980400085\n",
      "2400 \tCost:  0.5417236089706421\n",
      "2600 \tCost:  0.5368272662162781\n",
      "2800 \tCost:  0.532397985458374\n",
      "3000 \tCost:  0.5283803343772888\n",
      "3200 \tCost:  0.5247262716293335\n",
      "3400 \tCost:  0.5213930606842041\n",
      "3600 \tCost:  0.5183457732200623\n",
      "3800 \tCost:  0.5155525207519531\n",
      "4000 \tCost:  0.5129875540733337\n",
      "4200 \tCost:  0.510626494884491\n",
      "4400 \tCost:  0.5084487795829773\n",
      "4600 \tCost:  0.5064354538917542\n",
      "4800 \tCost:  0.504572331905365\n",
      "5000 \tCost:  0.5028441548347473\n",
      "5200 \tCost:  0.5012380480766296\n",
      "5400 \tCost:  0.4997446835041046\n",
      "5600 \tCost:  0.4983522593975067\n",
      "5800 \tCost:  0.4970538318157196\n",
      "6000 \tCost:  0.49583950638771057\n",
      "6200 \tCost:  0.49470454454421997\n",
      "6400 \tCost:  0.49364086985588074\n",
      "6600 \tCost:  0.4926432967185974\n",
      "6800 \tCost:  0.4917067289352417\n",
      "7000 \tCost:  0.4908266067504883\n",
      "7200 \tCost:  0.4899984300136566\n",
      "7400 \tCost:  0.489218533039093\n",
      "7600 \tCost:  0.48848336935043335\n",
      "7800 \tCost:  0.4877903461456299\n",
      "8000 \tCost:  0.487135648727417\n",
      "8200 \tCost:  0.48651641607284546\n",
      "8400 \tCost:  0.485930860042572\n",
      "8600 \tCost:  0.4853770136833191\n",
      "8800 \tCost:  0.48485174775123596\n",
      "9000 \tCost:  0.4843541383743286\n",
      "9200 \tCost:  0.48388242721557617\n",
      "9400 \tCost:  0.4834340512752533\n",
      "9600 \tCost:  0.48300793766975403\n",
      "9800 \tCost:  0.4826030135154724\n",
      "10000 \tCost:  0.4822181463241577\n"
     ]
    }
   ],
   "source": [
    "model = LogisticClassificationModel(input_size, num_classes)\n",
    "\n",
    "criterion = nn.BCELoss() # binary cost entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(epoch, '\\tCost: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7694])\n"
     ]
    }
   ],
   "source": [
    "predicted = (model(x_data) > 0.5).float()\n",
    "accuarcy = torch.mean(torch.eq(y_data, predicted).float())\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
