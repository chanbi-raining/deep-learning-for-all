{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training/test dataset, learning rate, normalization\n",
    "\n",
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# test data\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder('float', [None, 3])\n",
    "Y = tf.placeholder('float', [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.081255 [[ 0.13583648 -0.38768765  0.13107213]\n",
      " [ 1.2640156   0.25108227  0.11305523]\n",
      " [-1.138201   -0.6354152   1.051393  ]]\n",
      "20 1.1196831 [[-0.08479314 -0.3130521   0.27706623]\n",
      " [ 1.1134173   0.52133304 -0.00659726]\n",
      " [-0.73146725 -0.08336414  0.09260829]]\n",
      "40 0.9096384 [[-0.27067417 -0.34074715  0.49064234]\n",
      " [ 0.9416875   0.4565288   0.22993684]\n",
      " [-0.480514   -0.01893142 -0.22277766]]\n",
      "60 0.805599 [[-0.42029938 -0.36904278  0.66856307]\n",
      " [ 0.8317414   0.44416612  0.3522457 ]\n",
      " [-0.3049622   0.00357615 -0.42083693]]\n",
      "80 0.7462896 [[-0.5491432  -0.39119464  0.8195588 ]\n",
      " [ 0.7664158   0.4490647   0.41267267]\n",
      " [-0.18447335  0.01050187 -0.54825145]]\n",
      "100 0.7067431 [[-0.66526926 -0.40673265  0.9512226 ]\n",
      " [ 0.72971314  0.45753232  0.44090754]\n",
      " [-0.09939905  0.01234195 -0.63516587]]\n",
      "120 0.6771265 [[-0.77247876 -0.41679192  1.0684913 ]\n",
      " [ 0.71009207  0.46591008  0.45215103]\n",
      " [-0.03601373  0.01225056 -0.6984597 ]]\n",
      "140 0.6532589 [[-0.8727738  -0.4225925   1.1745868 ]\n",
      " [ 0.70021766  0.47354227  0.45439345]\n",
      " [ 0.01424172  0.01110346 -0.7475679 ]]\n",
      "160 0.6331442 [[-0.9674013  -0.4251081   1.2717296 ]\n",
      " [ 0.6957289   0.48045126  0.4519733 ]\n",
      " [ 0.05646491  0.00920305 -0.7878906 ]]\n",
      "180 0.61569977 [[-1.057248   -0.4250648   1.3615326 ]\n",
      " [ 0.6941214   0.48674372  0.44728842]\n",
      " [ 0.09364828  0.00672332 -0.8225941 ]]\n",
      "200 0.6002668 [[-1.1429929  -0.42300123  1.4452136 ]\n",
      " [ 0.6939835   0.49250352  0.4416665 ]\n",
      " [ 0.12755002  0.00380122 -0.8535737 ]]\n",
      "Prediction:  [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# correct prediction test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# lauch graph\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val)\n",
    "    \n",
    "    print('Prediction: ', sess.run(prediction, feed_dict = {X: x_test}))\n",
    "    print('Accuracy: ', sess.run(accuracy, feed_dict = {X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with a learning rate too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.547776 [[ 0.16383046  0.74922574  0.4181232 ]\n",
      " [-0.6922212  -1.2154421   0.6945758 ]\n",
      " [-0.41564     2.3490064  -1.9734311 ]]\n",
      "1 nan [[  2.5918891  -4.7040043   3.4432943]\n",
      " [ 15.401256  -25.74788     9.13354  ]\n",
      " [ 15.749678  -21.73063     5.9408884]]\n",
      "2 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Prediction:  [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder('float', [None, 3])\n",
    "Y = tf.placeholder('float', [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 10).minimize(cost)\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step in range(3):\n",
    "            print(step, cost_val, W_val)\n",
    "    \n",
    "    print('Prediction: ', sess.run(prediction, feed_dict = {X: x_test}))\n",
    "    print('Accuracy: ', sess.run(accuracy, feed_dict = {X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "20 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "40 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "60 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "80 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "100 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "120 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "140 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "160 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "180 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "200 8.336342 [[ 0.14300556  1.3906915  -0.55591047]\n",
      " [ 1.4210382  -0.26613915  0.6132015 ]\n",
      " [ 0.43897402 -1.5544853   0.79460126]]\n",
      "Prediction:  [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder('float', [None, 3])\n",
    "Y = tf.placeholder('float', [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-10).minimize(cost)\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val)\n",
    "    \n",
    "    print('Prediction: ', sess.run(prediction, feed_dict = {X: x_test}))\n",
    "    print('Accuracy: ', sess.run(accuracy, feed_dict = {X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-normalized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  18062332000.0 \n",
      "Prediction: \n",
      " [[ -93872.016]\n",
      " [-190137.45 ]\n",
      " [-149336.73 ]\n",
      " [-104352.29 ]\n",
      " [-123180.48 ]\n",
      " [-124227.77 ]\n",
      " [-113774.76 ]\n",
      " [-145167.39 ]]\n",
      "1 Cost:  1.9844726e+25 \n",
      "Prediction: \n",
      " [[3.1423450e+12]\n",
      " [6.3258624e+12]\n",
      " [4.9763278e+12]\n",
      " [3.4883793e+12]\n",
      " [4.1112416e+12]\n",
      " [4.1458451e+12]\n",
      " [3.7998103e+12]\n",
      " [4.8379142e+12]]\n",
      "2 Cost:  inf \n",
      "Prediction: \n",
      " [[-1.0415727e+20]\n",
      " [-2.0967924e+20]\n",
      " [-1.6494710e+20]\n",
      " [-1.1562704e+20]\n",
      " [-1.3627265e+20]\n",
      " [-1.3741963e+20]\n",
      " [-1.2594985e+20]\n",
      " [-1.6035920e+20]]\n",
      "3 Cost:  inf \n",
      "Prediction: \n",
      " [[3.4524333e+27]\n",
      " [6.9501021e+27]\n",
      " [5.4673948e+27]\n",
      " [3.8326148e+27]\n",
      " [4.5169412e+27]\n",
      " [4.5549595e+27]\n",
      " [4.1747777e+27]\n",
      " [5.3153222e+27]]\n",
      "4 Cost:  inf \n",
      "Prediction: \n",
      " [[-1.1443555e+35]\n",
      " [-2.3037050e+35]\n",
      " [-1.8122416e+35]\n",
      " [-1.2703717e+35]\n",
      " [-1.4972009e+35]\n",
      " [-1.5098026e+35]\n",
      " [-1.3837863e+35]\n",
      " [-1.7618349e+35]]\n",
      "5 Cost:  inf \n",
      "Prediction: \n",
      " [[inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]]\n",
      "6 Cost:  nan \n",
      "Prediction: \n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder('float', [None, 4])\n",
    "Y = tf.placeholder('float', [None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], \\\n",
    "                                   feed_dict = {X: x_data, Y: y_data})\n",
    "    if step in range(7):\n",
    "        print(step, 'Cost: ', cost_val, '\\nPrediction: \\n', hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999999 , 0.9999999 , 0.        , 0.9999999 , 0.9999999 ],\n",
       "       [0.70548484, 0.70439545, 0.9999999 , 0.71881775, 0.83755783],\n",
       "       [0.54412544, 0.50274819, 0.5760869 , 0.60646794, 0.66063303],\n",
       "       [0.33890349, 0.3136802 , 0.10869564, 0.4598913 , 0.43800914],\n",
       "       [0.51435995, 0.42582385, 0.3043478 , 0.585048  , 0.42624397],\n",
       "       [0.49556174, 0.42582385, 0.31521736, 0.48131129, 0.49276132],\n",
       "       [0.11436063, 0.        , 0.20652172, 0.22007774, 0.18597236],\n",
       "       [0.        , 0.07747099, 0.53260864, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = MinMaxScaler(xy)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.15934879 \n",
      "Prediction: \n",
      " [[ 0.35976   ]\n",
      " [ 0.647148  ]\n",
      " [ 0.29456604]\n",
      " [ 0.01287292]\n",
      " [ 0.0657032 ]\n",
      " [ 0.11226761]\n",
      " [-0.23065524]\n",
      " [ 0.25600368]]\n",
      "1 Cost:  0.15934011 \n",
      "Prediction: \n",
      " [[ 0.35977784]\n",
      " [ 0.6471641 ]\n",
      " [ 0.29457965]\n",
      " [ 0.01288375]\n",
      " [ 0.06571597]\n",
      " [ 0.11227983]\n",
      " [-0.23064725]\n",
      " [ 0.25601113]]\n",
      "2 Cost:  0.15933141 \n",
      "Prediction: \n",
      " [[ 0.35979575]\n",
      " [ 0.6471802 ]\n",
      " [ 0.29459327]\n",
      " [ 0.01289459]\n",
      " [ 0.06572868]\n",
      " [ 0.11229213]\n",
      " [-0.23063926]\n",
      " [ 0.25601852]]\n",
      "3 Cost:  0.15932271 \n",
      "Prediction: \n",
      " [[ 0.35981363]\n",
      " [ 0.6471965 ]\n",
      " [ 0.29460686]\n",
      " [ 0.01290545]\n",
      " [ 0.06574136]\n",
      " [ 0.11230433]\n",
      " [-0.23063126]\n",
      " [ 0.25602597]]\n",
      "4 Cost:  0.159314 \n",
      "Prediction: \n",
      " [[ 0.35983157]\n",
      " [ 0.64721256]\n",
      " [ 0.2946205 ]\n",
      " [ 0.01291619]\n",
      " [ 0.06575413]\n",
      " [ 0.11231656]\n",
      " [-0.23062328]\n",
      " [ 0.25603336]]\n",
      "5 Cost:  0.15930532 \n",
      "Prediction: \n",
      " [[ 0.35984942]\n",
      " [ 0.6472286 ]\n",
      " [ 0.29463425]\n",
      " [ 0.01292703]\n",
      " [ 0.06576681]\n",
      " [ 0.1123288 ]\n",
      " [-0.23061529]\n",
      " [ 0.2560408 ]]\n",
      "6 Cost:  0.15929662 \n",
      "Prediction: \n",
      " [[ 0.35986733]\n",
      " [ 0.6472447 ]\n",
      " [ 0.29464787]\n",
      " [ 0.01293787]\n",
      " [ 0.06577958]\n",
      " [ 0.11234106]\n",
      " [-0.23060729]\n",
      " [ 0.2560482 ]]\n",
      "2000 Cost:  0.14323977 \n",
      "Prediction: \n",
      " [[ 0.39414856]\n",
      " [ 0.678032  ]\n",
      " [ 0.32073823]\n",
      " [ 0.03368553]\n",
      " [ 0.09016395]\n",
      " [ 0.13577825]\n",
      " [-0.21527167]\n",
      " [ 0.27023008]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder('float', [None, 4])\n",
    "Y = tf.placeholder('float', [None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], \\\n",
    "                                   feed_dict = {X: x_data, Y: y_data})\n",
    "    if step in list(range(7))+[2000]:\n",
    "        print(step, 'Cost: ', cost_val, '\\nPrediction: \\n', hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  0001 cost = 2.583715345\n",
      "Epoch:  0002 cost = 1.100212263\n",
      "Epoch:  0003 cost = 0.875636507\n",
      "Epoch:  0004 cost = 0.761852958\n",
      "Epoch:  0005 cost = 0.689174994\n",
      "Epoch:  0006 cost = 0.636354402\n",
      "Epoch:  0007 cost = 0.596593750\n",
      "Epoch:  0008 cost = 0.565355184\n",
      "Epoch:  0009 cost = 0.539977395\n",
      "Epoch:  0010 cost = 0.519003762\n",
      "Epoch:  0011 cost = 0.501282826\n",
      "Epoch:  0012 cost = 0.486014978\n",
      "Epoch:  0013 cost = 0.472779119\n",
      "Epoch:  0014 cost = 0.461226143\n",
      "Epoch:  0015 cost = 0.450410709\n",
      "Accuracy:  0.8925\n",
      "Label:  [6]\n",
      "Prediction:  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# test model (one hot == one hot)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # one pass  of all training examples\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict = {X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch: ', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "    # session.run() or ____.eval\n",
    "    print('Accuracy: ', accuracy.eval(session = sess, \\\n",
    "                                 feed_dict = {X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print('Label: ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction: ', sess.run(tf.argmax(hypothesis, 1), \\\n",
    "                               feed_dict ={X: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap = 'Greys',\\\n",
    "          interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, nb_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "x_data = torch.from_numpy(np.array([[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]], dtype = np.float32))\n",
    "_, y_data = torch.max(torch.from_numpy(np.array([[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]])), 1) # one-hot\n",
    "\n",
    "# test data\n",
    "x_test = torch.from_numpy(np.array([[2, 1, 1], [3, 1, 2], [3, 3, 4]], dtype = np.float32))\n",
    "_, y_test = torch.max(torch.from_numpy(np.array([[0, 0, 1], [0, 0, 1], [0, 0, 1]])), 1)\n",
    "\n",
    "input_size = 3\n",
    "nb_classes = 3\n",
    "num_epochs = 201\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  tensor(1.3186, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.9118, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.8203, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.7632, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.7219, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.6897, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.6634, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.6414, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.6225, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.6060, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(0.5913, grad_fn=<NllLossBackward>)\n",
      "Prediction:  tensor([2, 2, 2])\n",
      "Accuracy:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionModel(input_size, nb_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(x_data)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    accuracy = torch.mean(torch.eq(y_data, predicted).float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print('Cost: ', loss)\n",
    "\n",
    "_, pred = torch.max(model(x_test), 1)\n",
    "print('Prediction: ', pred)\n",
    "print('Accuracy: ', torch.mean(torch.eq(y_test, pred).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a learning rate too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  tensor(3.6035, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(122.5742, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(67.4469, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(73.2545, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(67.5036, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(34.6190, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(49.6436, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(31.9465, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(31.7522, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(90.6025, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(30.4145, grad_fn=<NllLossBackward>)\n",
      "Prediction:  tensor([2, 2, 2])\n",
      "Accuracy:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 10.0\n",
    "\n",
    "model = LogisticRegressionModel(input_size, nb_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(x_data)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    accuracy = torch.mean(torch.eq(y_data, predicted).float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print('Cost: ', loss)\n",
    "\n",
    "_, pred = torch.max(model(x_test), 1)\n",
    "print('Prediction: ', pred)\n",
    "print('Accuracy: ', torch.mean(torch.eq(y_test, pred).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Cost:  tensor(2.4857, grad_fn=<NllLossBackward>)\n",
      "Prediction:  tensor([0, 0, 0])\n",
      "Accuracy:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-10\n",
    "\n",
    "model = LogisticRegressionModel(input_size, nb_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(x_data)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    accuracy = torch.mean(torch.eq(y_data, predicted).float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print('Cost: ', loss)\n",
    "\n",
    "_, pred = torch.max(model(x_test), 1)\n",
    "print('Prediction: ', pred)\n",
    "print('Accuracy: ', torch.mean(torch.eq(y_test, pred).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-normalized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = torch.Tensor(xy[:, 0:-1])\n",
    "y_data = torch.Tensor(xy[:, [-1]])\n",
    "\n",
    "input_size = 4\n",
    "nb_classes = 1\n",
    "num_epochs = 2001\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, nb_classes):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, nb_classes)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  2243173023744.0 \tPrediction:  tensor([[374123.5625],\n",
      "        [753039.5000],\n",
      "        [592411.5625],\n",
      "        [415308.4375],\n",
      "        [489445.2500],\n",
      "        [493563.2188],\n",
      "        [452376.0625],\n",
      "        [575933.5000]], grad_fn=<ThAddmmBackward>)\n",
      "1 Cost:  1.577299211714007e+29 \tPrediction:  tensor([[ -99047465549824.],\n",
      "        [-199392699088896.],\n",
      "        [-156855040999424.],\n",
      "        [-109954551775232.],\n",
      "        [-129587317047296.],\n",
      "        [-130678029025280.],\n",
      "        [-119770934411264.],\n",
      "        [-152492209864704.]], grad_fn=<ThAddmmBackward>)\n",
      "2 Cost:  inf \tPrediction:  tensor([[26264493127266094546944.],\n",
      "        [52873119812858450804736.],\n",
      "        [41593377164548542300160.],\n",
      "        [29156734081361004986368.],\n",
      "        [34362771401611545673728.],\n",
      "        [34651992569681278926848.],\n",
      "        [31759751615586368487424.],\n",
      "        [40436478981470727176192.]], grad_fn=<ThAddmmBackward>)\n",
      "3 Cost:  inf \tPrediction:  tensor([[ -6964576916544552934202815807488.],\n",
      "        [-14020407180731076922399977897984.],\n",
      "        [-11029346990909000993688216666112.],\n",
      "        [ -7731514016341885416805127684096.],\n",
      "        [ -9112002730258395268895968591872.],\n",
      "        [ -9188696379791837536424741044224.],\n",
      "        [ -8421758675531595246507841814528.],\n",
      "        [-10722572392775231923573126856704.]], grad_fn=<ThAddmmBackward>)\n",
      "4 Cost:  inf \tPrediction:  tensor([[inf],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [inf],\n",
      "        [inf]], grad_fn=<ThAddmmBackward>)\n",
      "5 Cost:  inf \tPrediction:  tensor([[-inf],\n",
      "        [-inf],\n",
      "        [-inf],\n",
      "        [-inf],\n",
      "        [-inf],\n",
      "        [-inf],\n",
      "        [-inf],\n",
      "        [-inf]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel(input_size, nb_classes)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_data, y_pred)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch in range(6):\n",
    "        print(epoch, 'Cost: ', loss.item(), '\\tPrediction: ', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "`X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))`  \n",
    "`X_scaled = X_std * (max - min) + min`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = MinMaxScaler(xy)\n",
    "x_data = torch.Tensor(xy[:, 0:-1])\n",
    "y_data = torch.Tensor(xy[:, [-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.7784412503242493 \tPrediction:  tensor([[0.4871],\n",
      "        [0.2612],\n",
      "        [0.3287],\n",
      "        [0.3791],\n",
      "        [0.3859],\n",
      "        [0.3377],\n",
      "        [0.3348],\n",
      "        [0.1475]], grad_fn=<ThAddmmBackward>)\n",
      "1 Cost:  0.7781715393066406 \tPrediction:  tensor([[0.4872],\n",
      "        [0.2613],\n",
      "        [0.3288],\n",
      "        [0.3792],\n",
      "        [0.3859],\n",
      "        [0.3377],\n",
      "        [0.3349],\n",
      "        [0.1475]], grad_fn=<ThAddmmBackward>)\n",
      "2 Cost:  0.777901828289032 \tPrediction:  tensor([[0.4873],\n",
      "        [0.2614],\n",
      "        [0.3289],\n",
      "        [0.3792],\n",
      "        [0.3860],\n",
      "        [0.3378],\n",
      "        [0.3349],\n",
      "        [0.1475]], grad_fn=<ThAddmmBackward>)\n",
      "3 Cost:  0.7776325345039368 \tPrediction:  tensor([[0.4874],\n",
      "        [0.2615],\n",
      "        [0.3290],\n",
      "        [0.3793],\n",
      "        [0.3861],\n",
      "        [0.3379],\n",
      "        [0.3350],\n",
      "        [0.1476]], grad_fn=<ThAddmmBackward>)\n",
      "4 Cost:  0.7773631811141968 \tPrediction:  tensor([[0.4875],\n",
      "        [0.2616],\n",
      "        [0.3290],\n",
      "        [0.3793],\n",
      "        [0.3862],\n",
      "        [0.3379],\n",
      "        [0.3350],\n",
      "        [0.1476]], grad_fn=<ThAddmmBackward>)\n",
      "2000 Cost:  0.45699387788772583 \tPrediction:  tensor([[0.6390],\n",
      "        [0.4020],\n",
      "        [0.4420],\n",
      "        [0.4615],\n",
      "        [0.4883],\n",
      "        [0.4357],\n",
      "        [0.3898],\n",
      "        [0.1997]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel(input_size, nb_classes)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_data, y_pred)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch in list(range(5))+[2000]:\n",
    "        print(epoch, 'Cost: ', loss.item(), '\\tPrediction: ', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root = './mnist_data/', train = True, \n",
    "                               transform = transforms.ToTensor(), download = True)\n",
    "test_dataset = datasets.MNIST(root = './mnist_data/', train = False,\n",
    "                             transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size,\n",
    "                                         shuffle = False)\n",
    "\n",
    "input_size = 784\n",
    "nb_classes = 10\n",
    "num_epochs = 15\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_xs.reshape(100, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tCost:  tensor(0.5370, grad_fn=<ThAddBackward>)\n",
      "1 \tCost:  tensor(0.3593, grad_fn=<ThAddBackward>)\n",
      "2 \tCost:  tensor(0.3312, grad_fn=<ThAddBackward>)\n",
      "3 \tCost:  tensor(0.3167, grad_fn=<ThAddBackward>)\n",
      "4 \tCost:  tensor(0.3069, grad_fn=<ThAddBackward>)\n",
      "5 \tCost:  tensor(0.3002, grad_fn=<ThAddBackward>)\n",
      "6 \tCost:  tensor(0.2950, grad_fn=<ThAddBackward>)\n",
      "7 \tCost:  tensor(0.2906, grad_fn=<ThAddBackward>)\n",
      "8 \tCost:  tensor(0.2874, grad_fn=<ThAddBackward>)\n",
      "9 \tCost:  tensor(0.2844, grad_fn=<ThAddBackward>)\n",
      "10 \tCost:  tensor(0.2818, grad_fn=<ThAddBackward>)\n",
      "11 \tCost:  tensor(0.2798, grad_fn=<ThAddBackward>)\n",
      "12 \tCost:  tensor(0.2777, grad_fn=<ThAddBackward>)\n",
      "13 \tCost:  tensor(0.2759, grad_fn=<ThAddBackward>)\n",
      "14 \tCost:  tensor(0.2745, grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionModel(input_size, nb_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch_xs, batch_ys = batch\n",
    "        \n",
    "        y_pred = model(batch_xs.reshape(100, -1))\n",
    "        loss = criterion(y_pred, batch_ys)\n",
    "        avg_cost+= loss / len(train_loader)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, '\\tCost: ', avg_cost)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92 %\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "for images, labels in test_loader:\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
