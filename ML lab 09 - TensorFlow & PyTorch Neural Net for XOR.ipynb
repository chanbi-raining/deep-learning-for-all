{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net for XOR\n",
    "\n",
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7842032 [[-1.0683634]\n",
      " [-0.9822596]]\n",
      "1000 0.6931846 [[-0.02375008]\n",
      " [-0.02358437]]\n",
      "2000 0.6931472 [[-0.00046466]\n",
      " [-0.00046434]]\n",
      "3000 0.6931472 [[-9.092030e-06]\n",
      " [-9.092101e-06]]\n",
      "4000 0.6931472 [[-1.8261983e-07]\n",
      " [-1.8269077e-07]]\n",
      "5000 0.6931472 [[-1.2301496e-07]\n",
      " [-1.2308590e-07]]\n",
      "6000 0.6931472 [[-1.2301496e-07]\n",
      " [-1.2308590e-07]]\n",
      "7000 0.6931472 [[-1.2301496e-07]\n",
      " [-1.2308590e-07]]\n",
      "8000 0.6931472 [[-1.2301496e-07]\n",
      " [-1.2308590e-07]]\n",
      "9000 0.6931472 [[-1.2301496e-07]\n",
      " [-1.2308590e-07]]\n",
      "10000 0.6931472 [[-1.2301496e-07]\n",
      " [-1.2308590e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    " \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add another layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7393449 [array([[-0.40783137, -0.13293551],\n",
      "       [ 1.3667812 , -0.50756603]], dtype=float32), array([[0.9981114],\n",
      "       [1.0434862]], dtype=float32)]\n",
      "1000 0.6882913 [array([[ 0.23466362, -0.5065814 ],\n",
      "       [ 1.2181981 , -0.76280856]], dtype=float32), array([[0.62582904],\n",
      "       [1.0350277 ]], dtype=float32)]\n",
      "2000 0.6488782 [array([[ 1.4867615 , -0.98474705],\n",
      "       [ 1.8402478 , -1.1237056 ]], dtype=float32), array([[1.3353385],\n",
      "       [1.1330584]], dtype=float32)]\n",
      "3000 0.4642966 [array([[ 3.4109287, -1.9247049],\n",
      "       [ 3.4799266, -1.9695876]], dtype=float32), array([[3.064374 ],\n",
      "       [2.1070573]], dtype=float32)]\n",
      "4000 0.23101728 [array([[ 4.7695613, -3.1464446],\n",
      "       [ 4.7909327, -3.1526823]], dtype=float32), array([[4.5367656],\n",
      "       [4.132371 ]], dtype=float32)]\n",
      "5000 0.123449035 [array([[ 5.4832067, -3.9343789],\n",
      "       [ 5.4939313, -3.9366803]], dtype=float32), array([[5.6427646],\n",
      "       [5.5267763]], dtype=float32)]\n",
      "6000 0.079254985 [array([[ 5.9038677, -4.4029   ],\n",
      "       [ 5.910891 , -4.404316 ]], dtype=float32), array([[6.434653 ],\n",
      "       [6.4134097]], dtype=float32)]\n",
      "7000 0.057165064 [array([[ 6.186975, -4.715564],\n",
      "       [ 6.192235, -4.7166  ]], dtype=float32), array([[7.0249276],\n",
      "       [7.0430074]], dtype=float32)]\n",
      "8000 0.044301815 [array([[ 6.3952894, -4.9438496],\n",
      "       [ 6.3995366, -4.9446764]], dtype=float32), array([[7.4885626],\n",
      "       [7.5265684]], dtype=float32)]\n",
      "9000 0.035994872 [array([[ 6.5579195, -5.1209908],\n",
      "       [ 6.561507 , -5.1216846]], dtype=float32), array([[7.867965],\n",
      "       [7.917517]], dtype=float32)]\n",
      "10000 0.030228801 [array([[ 6.690215 , -5.2643995],\n",
      "       [ 6.6933393, -5.264999 ]], dtype=float32), array([[8.188066],\n",
      "       [8.244946]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.03264424]\n",
      " [0.9747423 ]\n",
      " [0.9747354 ]\n",
      " [0.03589475]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    " \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9939028 [array([[-0.4443422 , -0.3159796 ,  0.4567972 , -0.0536186 ,  1.9479954 ,\n",
      "         0.06181101,  0.20378429,  0.7403946 ,  0.265544  ,  1.0427778 ],\n",
      "       [ 1.2180812 , -0.28995162,  0.34645423, -1.8682728 ,  0.5514891 ,\n",
      "         0.7790138 , -0.8983711 , -0.3669839 ,  1.7658756 , -1.654736  ]],\n",
      "      dtype=float32), array([[ 1.489721  ],\n",
      "       [ 1.1808524 ],\n",
      "       [-2.0710294 ],\n",
      "       [-0.69324046],\n",
      "       [-0.48182037],\n",
      "       [ 1.1392612 ],\n",
      "       [ 1.5554721 ],\n",
      "       [ 0.44341543],\n",
      "       [-0.34744713],\n",
      "       [-1.0540942 ]], dtype=float32)]\n",
      "1000 0.32917595 [array([[-1.8079867 ,  0.47983804,  0.62095267, -1.6134955 ,  1.7596561 ,\n",
      "         1.287326  ,  1.2708088 ,  1.3454975 , -0.35913303,  3.2194219 ],\n",
      "       [ 1.948204  , -0.548644  ,  1.3227661 , -2.133228  ,  0.5620352 ,\n",
      "         1.1659422 , -1.9513369 , -1.1009203 ,  1.9139912 , -3.3564663 ]],\n",
      "      dtype=float32), array([[ 2.0955653 ],\n",
      "       [ 1.2157556 ],\n",
      "       [-2.3611107 ],\n",
      "       [-1.8595686 ],\n",
      "       [ 0.16633657],\n",
      "       [ 1.624884  ],\n",
      "       [ 2.3349855 ],\n",
      "       [ 1.4655024 ],\n",
      "       [-0.86391044],\n",
      "       [-3.5597637 ]], dtype=float32)]\n",
      "2000 0.07920291 [array([[-2.8023527 ,  0.71453625,  1.114706  , -2.5065045 ,  1.8309591 ,\n",
      "         1.9770668 ,  2.48181   ,  1.9795719 , -0.58987767,  4.45722   ],\n",
      "       [ 2.7350729 , -0.9577198 ,  1.8175555 , -2.6104746 ,  0.5262372 ,\n",
      "         1.7187244 , -3.23201   , -2.1565304 ,  2.1823063 , -4.2966447 ]],\n",
      "      dtype=float32), array([[ 3.1342595],\n",
      "       [ 1.5791268],\n",
      "       [-2.8036938],\n",
      "       [-2.9053106],\n",
      "       [ 0.2939718],\n",
      "       [ 2.4660513],\n",
      "       [ 3.9663663],\n",
      "       [ 2.5947616],\n",
      "       [-1.2187191],\n",
      "       [-5.33643  ]], dtype=float32)]\n",
      "3000 0.03459082 [array([[-3.2378292 ,  0.7756765 ,  1.3782867 , -2.8347795 ,  1.8476226 ,\n",
      "         2.2304325 ,  2.9537609 ,  2.2906609 , -0.67717767,  4.8852835 ],\n",
      "       [ 3.063242  , -1.1645538 ,  1.9911023 , -2.8645504 ,  0.5162057 ,\n",
      "         1.9787668 , -3.7394543 , -2.6277056 ,  2.3114457 , -4.6191926 ]],\n",
      "      dtype=float32), array([[ 3.6943188 ],\n",
      "       [ 1.7278209 ],\n",
      "       [-3.076188  ],\n",
      "       [-3.331555  ],\n",
      "       [ 0.27504095],\n",
      "       [ 2.816728  ],\n",
      "       [ 4.722113  ],\n",
      "       [ 3.1271238 ],\n",
      "       [-1.357342  ],\n",
      "       [-6.09644   ]], dtype=float32)]\n",
      "4000 0.020729145 [array([[-3.4825335 ,  0.81400436,  1.537333  , -3.0083852 ,  1.8535752 ,\n",
      "         2.3682504 ,  3.2006292 ,  2.4717207 , -0.72896916,  5.108316  ],\n",
      "       [ 3.2438505 , -1.2871478 ,  2.0872054 , -3.0139606 ,  0.5122407 ,\n",
      "         2.128923  , -4.007246  , -2.8883681 ,  2.3870804 , -4.7870016 ]],\n",
      "      dtype=float32), array([[ 4.0362234 ],\n",
      "       [ 1.8143643 ],\n",
      "       [-3.2541332 ],\n",
      "       [-3.5681365 ],\n",
      "       [ 0.25203186],\n",
      "       [ 3.0160549 ],\n",
      "       [ 5.1503444 ],\n",
      "       [ 3.4402344 ],\n",
      "       [-1.4350904 ],\n",
      "       [-6.526136  ]], dtype=float32)]\n",
      "5000 0.014425128 [array([[-3.6467843 ,  0.84329087,  1.6486257 , -3.1216521 ,  1.8564793 ,\n",
      "         2.4602127 ,  3.3600059 ,  2.5951245 , -0.7657197 ,  5.2527847 ],\n",
      "       [ 3.3642585 , -1.372203  ,  2.1529658 , -3.116334  ,  0.51014286,\n",
      "         2.2316418 , -4.180919  , -3.0618055 ,  2.439414  , -4.8957458 ]],\n",
      "      dtype=float32), array([[ 4.277193  ],\n",
      "       [ 1.8749312 ],\n",
      "       [-3.384956  ],\n",
      "       [-3.7270286 ],\n",
      "       [ 0.23236378],\n",
      "       [ 3.1523275 ],\n",
      "       [ 5.4399385 ],\n",
      "       [ 3.6570861 ],\n",
      "       [-1.4881403 ],\n",
      "       [-6.8173203 ]], dtype=float32)]\n",
      "6000 0.010918589 [array([[-3.7684238 ,  0.86729676,  1.7334349 , -3.2041414 ,  1.858147  ,\n",
      "         2.528367  ,  3.475093  ,  2.6873164 , -0.79420125,  5.357529  ],\n",
      "       [ 3.4532263 , -1.4367467 ,  2.2028062 , -3.1930974 ,  0.5088454 ,\n",
      "         2.3088217 , -4.306708  , -3.189513  ,  2.4791038 , -4.9746547 ]],\n",
      "      dtype=float32), array([[ 4.461823  ],\n",
      "       [ 1.9214561 ],\n",
      "       [-3.4882338 ],\n",
      "       [-3.8450947 ],\n",
      "       [ 0.21575691],\n",
      "       [ 3.254987  ],\n",
      "       [ 5.6558223 ],\n",
      "       [ 3.8214395 ],\n",
      "       [-1.5281439 ],\n",
      "       [-7.034922  ]], dtype=float32)]\n",
      "7000 0.008716492 [array([[-3.8641305,  0.8877401,  1.8016043, -3.2683187,  1.8592038,\n",
      "         2.5821426,  3.5640125,  2.7602735, -0.8174589,  5.4387703],\n",
      "       [ 3.5231917, -1.4885244,  2.2428873, -3.2540174,  0.5079638,\n",
      "         2.370258 , -4.404117 , -3.2895684,  2.5109408, -5.035912 ]],\n",
      "      dtype=float32), array([[ 4.6109    ],\n",
      "       [ 1.9592241 ],\n",
      "       [-3.5735486 ],\n",
      "       [-3.9383597 ],\n",
      "       [ 0.20151168],\n",
      "       [ 3.3369842 ],\n",
      "       [ 5.826656  ],\n",
      "       [ 3.953114  ],\n",
      "       [-1.560155  ],\n",
      "       [-7.207523  ]], dtype=float32)]\n",
      "8000 0.0072176466 [array([[-3.9425597,  0.9055905,  1.8584173, -3.3204844,  1.8599167,\n",
      "         2.6263642,  3.635872 ,  2.8203104, -0.8371184,  5.5046554],\n",
      "       [ 3.5805433, -1.5316452,  2.2763855, -3.3042614,  0.5073275,\n",
      "         2.4210942, -4.482976 , -3.3712888,  2.5374591, -5.085632 ]],\n",
      "      dtype=float32), array([[ 4.73563   ],\n",
      "       [ 1.9910188 ],\n",
      "       [-3.6462421 ],\n",
      "       [-4.015091  ],\n",
      "       [ 0.18907934],\n",
      "       [ 3.4050708 ],\n",
      "       [ 5.967355  ],\n",
      "       [ 4.06262   ],\n",
      "       [-1.5867908 ],\n",
      "       [-7.349988  ]], dtype=float32)]\n",
      "9000 0.006137453 [array([[-4.0087285 ,  0.92145836,  1.907016  , -3.3642213 ,  1.8604296 ,\n",
      "         2.6638103 ,  3.695824  ,  2.8711226 , -0.8541496 ,  5.5597973 ],\n",
      "       [ 3.6289637 , -1.5685304 ,  2.3051488 , -3.3468688 ,  0.50684637,\n",
      "         2.4643388 , -4.5488634 , -3.4400456 ,  2.5601463 , -5.1272836 ]],\n",
      "      dtype=float32), array([[ 4.8426976 ],\n",
      "       [ 2.0184824 ],\n",
      "       [-3.7095897 ],\n",
      "       [-4.0800743 ],\n",
      "       [ 0.17806488],\n",
      "       [ 3.4631863 ],\n",
      "       [ 6.0865865 ],\n",
      "       [ 4.1561584 ],\n",
      "       [-1.609573  ],\n",
      "       [-7.4709606 ]], dtype=float32)]\n",
      "10000 0.0053250864 [array([[-4.0657825 ,  0.9357582 ,  1.9494116 , -3.4017484 ,  1.8607979 ,\n",
      "         2.696212  ,  3.7470431 ,  2.9150474 , -0.86917543,  5.6070375 ],\n",
      "       [ 3.6707523 , -1.6007222 ,  2.3303425 , -3.383762  ,  0.506467  ,\n",
      "         2.501897  , -4.6052217 , -3.499193  ,  2.5799465 , -5.162991  ]],\n",
      "      dtype=float32), array([[ 4.936392  ],\n",
      "       [ 2.0426621 ],\n",
      "       [-3.765737  ],\n",
      "       [-4.1363087 ],\n",
      "       [ 0.16818397],\n",
      "       [ 3.5138223 ],\n",
      "       [ 6.1898084 ],\n",
      "       [ 4.2376733 ],\n",
      "       [-1.6294638 ],\n",
      "       [-7.575879  ]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.00412275]\n",
      " [0.9946402 ]\n",
      " [0.99448895]\n",
      " [0.00624896]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    " \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8894781\n",
      "1000 0.11084187\n",
      "2000 0.021813307\n",
      "3000 0.01056877\n",
      "4000 0.006689002\n",
      "5000 0.0047946805\n",
      "6000 0.003692116\n",
      "7000 0.0029782595\n",
      "8000 0.0024818573\n",
      "9000 0.0021183963\n",
      "10000 0.0018418801\n",
      "\n",
      "Hypothesis:  [[0.00194763]\n",
      " [0.99848723]\n",
      " [0.99766266]\n",
      " [0.00156275]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    " \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard for XOR NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From TF graph, decide which tensors you want to log  \n",
    "2. Merge all summaries  \n",
    "3. Create writer and add graph  \n",
    "4. Run summary merge and add_summary  \n",
    "5. Launch TensorBoard  \n",
    "@ terminal :  tensorboard --logdir=.logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.77945554 [array([[-0.8968156 , -0.020731  ],\n",
      "       [ 0.2217462 ,  0.34836546]], dtype=float32), array([[ 0.12948525],\n",
      "       [-0.58366394]], dtype=float32)]\n",
      "1000 0.6894698 [array([[-0.9524784 , -0.40464795],\n",
      "       [ 0.3908801 ,  0.5334407 ]], dtype=float32), array([[ 0.3303816],\n",
      "       [-1.0449716]], dtype=float32)]\n",
      "2000 0.6343074 [array([[-1.5606138, -1.3594517],\n",
      "       [ 1.2699487,  1.4954305]], dtype=float32), array([[ 1.3888401],\n",
      "       [-1.6287345]], dtype=float32)]\n",
      "3000 0.23308071 [array([[-3.7417781, -3.5650854],\n",
      "       [ 3.464263 ,  3.7469532]], dtype=float32), array([[ 4.64394  ],\n",
      "       [-4.2491145]], dtype=float32)]\n",
      "4000 0.08614047 [array([[-4.7859526, -4.650079 ],\n",
      "       [ 4.5250626,  4.8440742]], dtype=float32), array([[ 6.7147646],\n",
      "       [-6.1868567]], dtype=float32)]\n",
      "5000 0.048715986 [array([[-5.2556763, -5.1468196],\n",
      "       [ 5.001802 ,  5.3486404]], dtype=float32), array([[ 7.805442 ],\n",
      "       [-7.2550697]], dtype=float32)]\n",
      "6000 0.033275403 [array([[-5.539773 , -5.447521 ],\n",
      "       [ 5.2891307,  5.6547103]], dtype=float32), array([[ 8.521214 ],\n",
      "       [-7.9650064]], dtype=float32)]\n",
      "7000 0.025067862 [array([[-5.7386904, -5.657779 ],\n",
      "       [ 5.4897447,  5.868911 ]], dtype=float32), array([[ 9.049741],\n",
      "       [-8.491944]], dtype=float32)]\n",
      "8000 0.020030197 [array([[-5.8898892, -5.8173356],\n",
      "       [ 5.641896 ,  6.0315285]], dtype=float32), array([[ 9.467411],\n",
      "       [-8.909465]], dtype=float32)]\n",
      "9000 0.016641898 [array([[-6.0109487, -5.944886 ],\n",
      "       [ 5.7635093,  6.161558 ]], dtype=float32), array([[ 9.812156],\n",
      "       [-9.254617]], dtype=float32)]\n",
      "10000 0.0142146 [array([[-6.111403 , -6.0505767],\n",
      "       [ 5.8642783,  6.2693143]], dtype=float32), array([[10.105421],\n",
      "       [-9.548514]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.01380483]\n",
      " [0.9873767 ]\n",
      " [0.9816488 ]\n",
      " [0.0116635 ]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('layer1'):\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    w1_hist = tf.summary.histogram('weights1', W1)\n",
    "    b1_hist = tf.summary.histogram('biases1', b1)\n",
    "    layer1_hist = tf.summary.histogram('layer1', layer1)\n",
    "    \n",
    "with tf.name_scope('layer1'):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    \n",
    "    w2_hist = tf.summary.histogram('weights2', W2)\n",
    "    b2_hist = tf.summary.histogram('biases2', b2)\n",
    "    hypothesis_hist = tf.summary.histogram('hypothesis', hypothesis)\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    cost_summ = tf.summary.scalar('cost', cost)\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "     train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./logs/xor_logs_r0_01') # directory for the file\n",
    "    writer.add_graph(sess.graph) # show the graph\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict = {X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, global_step = step)\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    " \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.from_numpy(np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32))\n",
    "y_data = torch.from_numpy(np.array([[0], [1], [1], [0]], dtype = np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(2, 2)\n",
    "        self.l2 = nn.Linear(2, 1)\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        y_pred = self.sigmoid(self.l2(out1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  tensor(0.6938, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.5000)\n",
      "1000 Cost:  tensor(0.6916, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.5000)\n",
      "2000 Cost:  tensor(0.6744, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.7500)\n",
      "3000 Cost:  tensor(0.5774, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.7500)\n",
      "4000 Cost:  tensor(0.4850, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.7500)\n",
      "5000 Cost:  tensor(0.3052, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "6000 Cost:  tensor(0.1132, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "7000 Cost:  tensor(0.0586, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "8000 Cost:  tensor(0.0382, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "9000 Cost:  tensor(0.0280, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "10000 Cost:  tensor(0.0220, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.BCELoss(reduction='elementwise_mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    y_pred = model(x_data)\n",
    "    predicted = (model(x_data) > 0.5).float()\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    accuracy = torch.mean(torch.eq(y_data, predicted).float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, 'Cost: ', loss, '\\tAccuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(2, 10)\n",
    "        self.l2 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        y_pred = self.sigmoid(self.l2(out1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.BCELoss(reduction='elementwise_mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  tensor(0.6962, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.5000)\n",
      "1000 Cost:  tensor(0.6925, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.5000)\n",
      "2000 Cost:  tensor(0.6897, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.5000)\n",
      "3000 Cost:  tensor(0.6630, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.5000)\n",
      "4000 Cost:  tensor(0.4441, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "5000 Cost:  tensor(0.1401, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "6000 Cost:  tensor(0.0598, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "7000 Cost:  tensor(0.0352, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "8000 Cost:  tensor(0.0242, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "9000 Cost:  tensor(0.0182, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "10000 Cost:  tensor(0.0144, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10001):\n",
    "    y_pred = model(x_data)\n",
    "    predicted = (model(x_data) > 0.5).float()\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    accuracy = torch.mean(torch.eq(y_data, predicted).float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, 'Cost: ', loss, '\\tAccuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(2, 10)\n",
    "        self.l2 = nn.Linear(10, 10)\n",
    "        self.l3 = nn.Linear(10, 10)\n",
    "        self.l4 = nn.Linear(10, 1)\n",
    "        \n",
    "        init.normal_(self.l1.weight)\n",
    "        init.normal_(self.l2.weight)\n",
    "        init.normal_(self.l3.weight)\n",
    "        init.normal_(self.l4.weight)\n",
    "        init.normal_(self.l1.bias)\n",
    "        init.normal_(self.l2.bias)\n",
    "        init.normal_(self.l3.bias)\n",
    "        init.normal_(self.l4.bias)\n",
    "    \n",
    "        self.sigmoid = torch.sigmoid\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        out3 = self.sigmoid(self.l3(out2))\n",
    "        y_pred = self.sigmoid(self.l4(out3))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  tensor(0.8440, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(0.5000)\n",
      "1000 Cost:  tensor(0.2838, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "2000 Cost:  tensor(0.0204, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "3000 Cost:  tensor(0.0081, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "4000 Cost:  tensor(0.0048, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "5000 Cost:  tensor(0.0033, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "6000 Cost:  tensor(0.0025, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "7000 Cost:  tensor(0.0020, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "8000 Cost:  tensor(0.0017, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "9000 Cost:  tensor(0.0014, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n",
      "10000 Cost:  tensor(0.0012, grad_fn=<BinaryCrossEntropyBackward>) \tAccuracy:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.BCELoss(reduction='elementwise_mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    y_pred = model(x_data)\n",
    "    predicted = (model(x_data) > 0.5).float()\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    accuracy = torch.mean(torch.eq(y_data, predicted).float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, 'Cost: ', loss, '\\tAccuracy: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
