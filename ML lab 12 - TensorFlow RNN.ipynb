{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "array([[[1., 0., 0., 0.]]], dtype=float32)\n",
      "array([[[-0.5765048 , -0.36967593]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('one_cell') as scope:\n",
    "    # input_dim = 4, output_dim = 2\n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicRNNCell(num_units = hidden_size)\n",
    "    print(cell.output_size, cell.state_size)\n",
    "    \n",
    "    x_data = np.array([[h]], dtype = np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype = tf.float32)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 4)\n",
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)\n",
      "<bound method Tensor.eval of <tf.Tensor 'two_sequences/rnn/transpose_1:0' shape=(1, 5, 2) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('two_sequences') as scope:\n",
    "    # input_dim = 4, output_dim = 2, sequence = 5\n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicRNNCell(num_units = hidden_size)\n",
    "    x_data = np.array([[h, e, l, l, o]], dtype = np.float32)\n",
    "    print(x_data.shape)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype = tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5967595\n"
     ]
    }
   ],
   "source": [
    "y_data = tf.constant([[1, 1, 1]])\n",
    "prediction = tf.constant([[[0.2, 0.7], [0.6, 0.2], [0.2, 0.9]]], dtype=tf.float32)\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=prediction, targets=y_data, weights=weights)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Loss: \", sequence_loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1:  0.5130153 Loss2:  0.3711007\n"
     ]
    }
   ],
   "source": [
    "prediction1 = tf.constant([[[0.3, 0.7], [0.3, 0.7], [0.3, 0.7]]], dtype=tf.float32)\n",
    "prediction2 = tf.constant([[[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]]], dtype=tf.float32)\n",
    "sequence_loss1 = tf.contrib.seq2seq.sequence_loss(prediction1, y_data, weights)\n",
    "sequence_loss2 = tf.contrib.seq2seq.sequence_loss(prediction2, y_data, weights)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Loss1: \", sequence_loss1.eval(),\n",
    "      \"Loss2: \", sequence_loss2.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi Hello : hihell -> ihello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]\n",
    "x_one_hot = [[[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], \n",
    "              [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0]]]\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]\n",
    "\n",
    "num_classes = 5\n",
    "input_dim = 5\n",
    "hidden_size = 5\n",
    "batch_size = 1\n",
    "sequence_length = 6\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "cell = rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple = True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state = initial_state,\n",
    "                                    dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(6), Dimension(5)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, \n",
    "                                                weights = weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.6004362 prediction:  [[1 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ieelll\n",
      "1 loss:  1.5065582 prediction:  [[3 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  leelll\n",
      "2 loss:  1.4283648 prediction:  [[3 2 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  lellll\n",
      "3 loss:  1.3643515 prediction:  [[2 2 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eellll\n",
      "4 loss:  1.3051772 prediction:  [[2 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eeelll\n",
      "5 loss:  1.2452815 prediction:  [[2 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eeelll\n",
      "6 loss:  1.1890583 prediction:  [[2 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eeelll\n",
      "7 loss:  1.1351612 prediction:  [[2 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eeelll\n",
      "8 loss:  1.0822697 prediction:  [[2 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eeelll\n",
      "9 loss:  1.0367856 prediction:  [[2 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eeelll\n",
      "10 loss:  0.9985542 prediction:  [[2 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eeelll\n",
      "11 loss:  0.9648565 prediction:  [[2 2 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  eeelll\n",
      "12 loss:  0.9353928 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "13 loss:  0.90952146 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "14 loss:  0.88577604 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "15 loss:  0.865024 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "16 loss:  0.8467715 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "17 loss:  0.83056444 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "18 loss:  0.816681 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "19 loss:  0.80412424 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "20 loss:  0.7927489 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "21 loss:  0.78242195 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "22 loss:  0.7726362 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "23 loss:  0.7636608 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "24 loss:  0.7554173 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "25 loss:  0.7475009 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihelll\n",
      "26 loss:  0.73960006 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "27 loss:  0.7311294 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "28 loss:  0.72283304 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "29 loss:  0.7171134 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "30 loss:  0.71259487 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "31 loss:  0.70841 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "32 loss:  0.7076082 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "33 loss:  0.7061906 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "34 loss:  0.70236677 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "35 loss:  0.6994397 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "36 loss:  0.6957313 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "37 loss:  0.6916611 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "38 loss:  0.6894673 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "39 loss:  0.6872176 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "40 loss:  0.68468523 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "41 loss:  0.6833541 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "42 loss:  0.68162477 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "43 loss:  0.679218 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "44 loss:  0.6774489 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "45 loss:  0.67528456 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "46 loss:  0.6728344 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "47 loss:  0.6711023 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "48 loss:  0.6689606 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n",
      "49 loss:  0.66701615 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\tPreduction str:  ihello\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.argmax(outputs, axis = 2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict = {X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict = {X: x_one_hot})\n",
    "        print(i, 'loss: ', l, 'prediction: ', result, 'true Y: ', y_data, end = '')\n",
    "        \n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print('\\tPreduction str: ', ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Sequence RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 't', 'a', 'f', 'i', 'n', 'o', 'y', ' ', 'u']\n",
      "{'w': 0, 't': 1, 'a': 2, 'f': 3, 'i': 4, 'n': 5, 'o': 6, 'y': 7, ' ': 8, 'u': 9}\n",
      "[8, 4, 3, 8, 7, 6, 9, 8, 0, 2, 5, 1, 8, 7, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "sample = ' if you want you'\n",
    "idx2char = list(set(sample))\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)} # char -> idx\n",
    "\n",
    "dict_size = len(char2idx)\n",
    "hidden_size = len(char2idx)\n",
    "num_classes = len(char2idx)\n",
    "batch_size = 1\n",
    "sequence_length = len(sample) - 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]\n",
    "\n",
    "print(idx2char)\n",
    "print(char2idx)\n",
    "print(sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [sample_idx[:-1]]\n",
    "y_data = [sample_idx[1:]]\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "x_one_hot = tf.one_hot(X, num_classes)\n",
    "cell = rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state = initial_state, \n",
    "                                     dtype = tf.float32)\n",
    "\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, \n",
    "                                            activation_fn = None)\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(outputs, Y, weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "trian = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.3026102 Prediction: uuuuuuuuuuuuuuu\n",
      "1 loss: 2.1664429 Prediction: yyyyyu yyyoyyyu\n",
      "2 loss: 2.0629306 Prediction: yoyyou      oou\n",
      "3 loss: 1.7918866 Prediction: yyyyou      oou\n",
      "4 loss: 1.5047005 Prediction: yyyyou y nnyyou\n",
      "5 loss: 1.1587769 Prediction: yf you wnnn you\n",
      "6 loss: 0.78382695 Prediction: yf you want you\n",
      "7 loss: 0.4591326 Prediction: yf you wan  you\n",
      "8 loss: 0.3100561 Prediction: if you want you\n",
      "9 loss: 0.16840306 Prediction: if you want you\n",
      "10 loss: 0.11590148 Prediction: if you want you\n",
      "11 loss: 0.05929768 Prediction: if you want you\n",
      "12 loss: 0.024187865 Prediction: if you want you\n",
      "13 loss: 0.012777647 Prediction: if you want you\n",
      "14 loss: 0.007706294 Prediction: if you want you\n",
      "15 loss: 0.0018703882 Prediction: if you want you\n",
      "16 loss: 0.0010819497 Prediction: if you want you\n",
      "17 loss: 0.0016029682 Prediction: if you want you\n",
      "18 loss: 0.0011642732 Prediction: if you want you\n",
      "19 loss: 0.0005583805 Prediction: if you want you\n",
      "20 loss: 0.00034766676 Prediction: if you want you\n",
      "21 loss: 0.0002465238 Prediction: if you want you\n",
      "22 loss: 0.0001793145 Prediction: if you want you\n",
      "23 loss: 0.0001338079 Prediction: if you want you\n",
      "24 loss: 0.000104010796 Prediction: if you want you\n",
      "25 loss: 8.410088e-05 Prediction: if you want you\n",
      "26 loss: 6.962489e-05 Prediction: if you want you\n",
      "27 loss: 5.800166e-05 Prediction: if you want you\n",
      "28 loss: 4.8134574e-05 Prediction: if you want you\n",
      "29 loss: 3.9633956e-05 Prediction: if you want you\n",
      "30 loss: 3.243603e-05 Prediction: if you want you\n",
      "31 loss: 2.6493117e-05 Prediction: if you want you\n",
      "32 loss: 2.1717922e-05 Prediction: if you want you\n",
      "33 loss: 1.798342e-05 Prediction: if you want you\n",
      "34 loss: 1.5099015e-05 Prediction: if you want you\n",
      "35 loss: 1.2889956e-05 Prediction: if you want you\n",
      "36 loss: 1.12529915e-05 Prediction: if you want you\n",
      "37 loss: 1.002922e-05 Prediction: if you want you\n",
      "38 loss: 9.083562e-06 Prediction: if you want you\n",
      "39 loss: 8.39219e-06 Prediction: if you want you\n",
      "40 loss: 7.82796e-06 Prediction: if you want you\n",
      "41 loss: 7.4147183e-06 Prediction: if you want you\n",
      "42 loss: 7.0650485e-06 Prediction: if you want you\n",
      "43 loss: 6.7710066e-06 Prediction: if you want you\n",
      "44 loss: 6.5087515e-06 Prediction: if you want you\n",
      "45 loss: 6.2782847e-06 Prediction: if you want you\n",
      "46 loss: 6.0398697e-06 Prediction: if you want you\n",
      "47 loss: 5.817349e-06 Prediction: if you want you\n",
      "48 loss: 5.5868823e-06 Prediction: if you want you\n",
      "49 loss: 5.364361e-06 Prediction: if you want you\n"
     ]
    }
   ],
   "source": [
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, \n",
    "                                                 weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "idx2char = list(set(sentence))\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)} # char -> idx\n",
    "\n",
    "dict_size = len(char2idx)\n",
    "hidden_size = len(char2idx)\n",
    "num_classes = len(char2idx)\n",
    "batch_size = 1\n",
    "sequence_length = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    #print(i, x_str, '->', y_str)\n",
    "\n",
    "    x = [char2idx[c] for c in x_str]  # x str to index\n",
    "    y = [char2idx[c] for c in y_str]  # y str to index\n",
    "\n",
    "    dataX.append(x)\n",
    "    dataY.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10), Dimension(25)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = len(dataX)\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "X_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple=True)\n",
    "# how many layers = 2\n",
    "cell = rnn.MultiRNNCell([cell] * 2, state_is_tuple = True)\n",
    "\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, initial_state = initial_state, \n",
    "                                     dtype = tf.float32)\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "ouputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "# always - almost like an idiom\n",
    "X_for_softmax = tf.reshape(outputs, [-1, hidden_size])\n",
    "\n",
    "softmax_w = tf.get_variable('softmax_w', [hidden_size, num_classes])\n",
    "softmax_b = tf.get_variable('softmax_b', [num_classes])\n",
    "outputs = tf.matmul(X_for_softmax, softmax_w) + softmax_b\n",
    "\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, \n",
    "                                                 weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.178678\n",
      "50 0.30942163\n",
      "100 0.23328196\n",
      "150 0.23062876\n",
      "200 0.22993594\n",
      "250 0.22944461\n",
      "300 0.22945312\n",
      "350 0.22966756\n",
      "400 0.22894523\n",
      "450 0.22887793\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(500):\n",
    "    _, l, results = sess.run([train, loss, outputs], feed_dict = {X: dataX, Y: dataY})\n",
    "    for j, result in enumerate(results):\n",
    "        index = np.argmax(result, axis = 1)\n",
    "    if i % 50 == 0:\n",
    "        print(i, l)\n",
    "        \n",
    "results = sess.run(outputs, feed_dict = {X: dataX})\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result, axis = 1)\n",
    "    if j is 0: # print all for the first result to make a sentence\n",
    "        print(''.join([idx2char[t] for t in index]), end = '')\n",
    "    else:\n",
    "        print(idx2char[index[-1]], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
